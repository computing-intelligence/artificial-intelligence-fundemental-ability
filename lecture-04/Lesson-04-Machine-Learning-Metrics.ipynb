{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric For Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
      "0   0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
      "1   0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
      "2   0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
      "3   0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
      "4   0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
      "5   0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
      "6   0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
      "7   0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
      "8   0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
      "9   0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
      "10  0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467  5.0  311.0   \n",
      "11  0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267  5.0  311.0   \n",
      "12  0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509  5.0  311.0   \n",
      "13  0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075  4.0  307.0   \n",
      "14  0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619  4.0  307.0   \n",
      "15  0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986  4.0  307.0   \n",
      "16  1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986  4.0  307.0   \n",
      "17  0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579  4.0  307.0   \n",
      "18  0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965  4.0  307.0   \n",
      "19  0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965  4.0  307.0   \n",
      "\n",
      "    PTRATIO       B  LSTAT  price  expensive  \n",
      "0      15.3  396.90   4.98   24.0          1  \n",
      "1      17.8  396.90   9.14   21.6          0  \n",
      "2      17.8  392.83   4.03   34.7          1  \n",
      "3      18.7  394.63   2.94   33.4          1  \n",
      "4      18.7  396.90   5.33   36.2          1  \n",
      "5      18.7  394.12   5.21   28.7          1  \n",
      "6      15.2  395.60  12.43   22.9          0  \n",
      "7      15.2  396.90  19.15   27.1          1  \n",
      "8      15.2  386.63  29.93   16.5          0  \n",
      "9      15.2  386.71  17.10   18.9          0  \n",
      "10     15.2  392.52  20.45   15.0          0  \n",
      "11     15.2  396.90  13.27   18.9          0  \n",
      "12     15.2  390.50  15.71   21.7          0  \n",
      "13     21.0  396.90   8.26   20.4          0  \n",
      "14     21.0  380.02  10.26   18.2          0  \n",
      "15     21.0  395.62   8.47   19.9          0  \n",
      "16     21.0  386.85   6.58   23.1          0  \n",
      "17     21.0  386.75  14.67   17.5          0  \n",
      "18     21.0  288.99  11.69   20.2          0  \n",
      "19     21.0  390.95  11.28   18.2          0  \n",
      "Epoch: 0 Batch: 0, loss: 0.00014705991754622018\n",
      "Epoch: 0 Batch: 100, loss: 20.133443902620385\n",
      "Epoch: 0 Batch: 200, loss: 14.044399029267202\n",
      "Epoch: 0 Batch: 300, loss: 21.182479944303623\n",
      "Epoch: 0 Batch: 400, loss: 13.606762070631747\n",
      "Epoch: 0 Batch: 500, loss: 16.361921692803453\n",
      "Epoch: 1 Batch: 0, loss: 17.501925777164196\n",
      "Epoch: 1 Batch: 100, loss: 9.800967883026275e-06\n",
      "Epoch: 1 Batch: 200, loss: 12.67441252438463\n",
      "Epoch: 1 Batch: 300, loss: 3.6935137685243865e-06\n",
      "Epoch: 1 Batch: 400, loss: 1.8821473817149985e-06\n",
      "Epoch: 1 Batch: 500, loss: 26.145831858207654\n",
      "Epoch: 2 Batch: 0, loss: 0.0011641494600401175\n",
      "Epoch: 2 Batch: 100, loss: 16.6911462963563\n",
      "Epoch: 2 Batch: 200, loss: 11.609728587944476\n",
      "Epoch: 2 Batch: 300, loss: 1.1057751964886794e-06\n",
      "Epoch: 2 Batch: 400, loss: 7.843799237874824e-05\n",
      "Epoch: 2 Batch: 500, loss: 1.812019875104691e-05\n",
      "Epoch: 3 Batch: 0, loss: 29.66014675423315\n",
      "Epoch: 3 Batch: 100, loss: 2.544539832163775e-05\n",
      "Epoch: 3 Batch: 200, loss: 7.021090179996517e-05\n",
      "Epoch: 3 Batch: 300, loss: 8.07230389900667e-06\n",
      "Epoch: 3 Batch: 400, loss: 8.706748995381516e-05\n",
      "Epoch: 3 Batch: 500, loss: 11.438775298339458\n",
      "Epoch: 4 Batch: 0, loss: 12.238881086754297\n",
      "Epoch: 4 Batch: 100, loss: 22.631457253644232\n",
      "Epoch: 4 Batch: 200, loss: 14.832790850093478\n",
      "Epoch: 4 Batch: 300, loss: 13.19016880584226\n",
      "Epoch: 4 Batch: 400, loss: 6.922838192816468e-05\n",
      "Epoch: 4 Batch: 500, loss: 9.04301332750714\n",
      "Epoch: 5 Batch: 0, loss: 25.356058834408575\n",
      "Epoch: 5 Batch: 100, loss: 0.0001252981060891947\n",
      "Epoch: 5 Batch: 200, loss: 10.565573486287501\n",
      "Epoch: 5 Batch: 300, loss: 8.450379224631371\n",
      "Epoch: 5 Batch: 400, loss: 0.0004223481092264103\n",
      "Epoch: 5 Batch: 500, loss: 10.5424837654096\n",
      "Epoch: 6 Batch: 0, loss: 2.670909180751041e-05\n",
      "Epoch: 6 Batch: 100, loss: 18.12834876117409\n",
      "Epoch: 6 Batch: 200, loss: 14.72131877860883\n",
      "Epoch: 6 Batch: 300, loss: 0.00012325618393769984\n",
      "Epoch: 6 Batch: 400, loss: 10.320892789153362\n",
      "Epoch: 6 Batch: 500, loss: 13.329730994391822\n",
      "Epoch: 7 Batch: 0, loss: 19.008759615029398\n",
      "Epoch: 7 Batch: 100, loss: 10.463976039418599\n",
      "Epoch: 7 Batch: 200, loss: 12.427124808149372\n",
      "Epoch: 7 Batch: 300, loss: 0.00038565751719527605\n",
      "Epoch: 7 Batch: 400, loss: 0.00016742675430822814\n",
      "Epoch: 7 Batch: 500, loss: 0.0008793152643063062\n",
      "Epoch: 8 Batch: 0, loss: 10.145746536225271\n",
      "Epoch: 8 Batch: 100, loss: 13.518196006596362\n",
      "Epoch: 8 Batch: 200, loss: 3.608330392667122e-05\n",
      "Epoch: 8 Batch: 300, loss: 10.914492629294113\n",
      "Epoch: 8 Batch: 400, loss: 9.340401975833368\n",
      "Epoch: 8 Batch: 500, loss: 0.0007357921509546572\n",
      "Epoch: 9 Batch: 0, loss: 10.156314421997118\n",
      "Epoch: 9 Batch: 100, loss: 0.000419023823090098\n",
      "Epoch: 9 Batch: 200, loss: 10.879211918252565\n",
      "Epoch: 9 Batch: 300, loss: 8.533738542594259\n",
      "Epoch: 9 Batch: 400, loss: 0.0009092638666881426\n",
      "Epoch: 9 Batch: 500, loss: 0.00040272624061477567\n",
      "Epoch: 10 Batch: 0, loss: 9.717469911670417\n",
      "Epoch: 10 Batch: 100, loss: 7.280955641385689\n",
      "Epoch: 10 Batch: 200, loss: 6.5629068774972374\n",
      "Epoch: 10 Batch: 300, loss: 10.596168638221705\n",
      "Epoch: 10 Batch: 400, loss: 7.926475562403277\n",
      "Epoch: 10 Batch: 500, loss: 6.309393051118839\n",
      "Epoch: 11 Batch: 0, loss: 7.942030504819375\n",
      "Epoch: 11 Batch: 100, loss: 0.0026839717816704764\n",
      "Epoch: 11 Batch: 200, loss: 11.788394317607182\n",
      "Epoch: 11 Batch: 300, loss: 8.116184968171948\n",
      "Epoch: 11 Batch: 400, loss: 8.83270178696011\n",
      "Epoch: 11 Batch: 500, loss: 0.0025066531905892715\n",
      "Epoch: 12 Batch: 0, loss: 0.0012939197564143564\n",
      "Epoch: 12 Batch: 100, loss: 6.652965767196663\n",
      "Epoch: 12 Batch: 200, loss: 8.950519742111906\n",
      "Epoch: 12 Batch: 300, loss: 0.005226700127826242\n",
      "Epoch: 12 Batch: 400, loss: 0.006346104589520468\n",
      "Epoch: 12 Batch: 500, loss: 7.983388989016163\n",
      "Epoch: 13 Batch: 0, loss: 7.055215203639482\n",
      "Epoch: 13 Batch: 100, loss: 7.494249142187506\n",
      "Epoch: 13 Batch: 200, loss: 5.811000414119349\n",
      "Epoch: 13 Batch: 300, loss: 6.018170297415103\n",
      "Epoch: 13 Batch: 400, loss: 6.859316802588245\n",
      "Epoch: 13 Batch: 500, loss: 5.287654511242687\n",
      "Epoch: 14 Batch: 0, loss: 8.141684400837992\n",
      "Epoch: 14 Batch: 100, loss: 0.004647082624232512\n",
      "Epoch: 14 Batch: 200, loss: 7.140034026343557\n",
      "Epoch: 14 Batch: 300, loss: 0.007826601937301232\n",
      "Epoch: 14 Batch: 400, loss: 0.009163119058921384\n",
      "Epoch: 14 Batch: 500, loss: 4.599616915898015\n",
      "Epoch: 15 Batch: 0, loss: 0.015360011005036684\n",
      "Epoch: 15 Batch: 100, loss: 4.709113380631556\n",
      "Epoch: 15 Batch: 200, loss: 4.629982626293401\n",
      "Epoch: 15 Batch: 300, loss: 4.856635955540772\n",
      "Epoch: 15 Batch: 400, loss: 5.03433729237849\n",
      "Epoch: 15 Batch: 500, loss: 0.012348538362209885\n",
      "Epoch: 16 Batch: 0, loss: 4.05740116162479\n",
      "Epoch: 16 Batch: 100, loss: 3.8052708104973174\n",
      "Epoch: 16 Batch: 200, loss: 4.228559855351133\n",
      "Epoch: 16 Batch: 300, loss: 3.609224109374655\n",
      "Epoch: 16 Batch: 400, loss: 3.6405514240905394\n",
      "Epoch: 16 Batch: 500, loss: 3.347174762027242\n",
      "Epoch: 17 Batch: 0, loss: 3.2592310877726733\n",
      "Epoch: 17 Batch: 100, loss: 0.03322369742681881\n",
      "Epoch: 17 Batch: 200, loss: 2.97365307924595\n",
      "Epoch: 17 Batch: 300, loss: 2.895783136985747\n",
      "Epoch: 17 Batch: 400, loss: 2.5625978865846273\n",
      "Epoch: 17 Batch: 500, loss: 0.054582620822394484\n",
      "Epoch: 18 Batch: 0, loss: 2.7179712669423366\n",
      "Epoch: 18 Batch: 100, loss: 2.295937483193501\n",
      "Epoch: 18 Batch: 200, loss: 2.145601055305654\n",
      "Epoch: 18 Batch: 300, loss: 0.055135326175882564\n",
      "Epoch: 18 Batch: 400, loss: 1.9529927528551403\n",
      "Epoch: 18 Batch: 500, loss: 0.09994743401925067\n",
      "Epoch: 19 Batch: 0, loss: 1.7109214844746272\n",
      "Epoch: 19 Batch: 100, loss: 1.5397174027793878\n",
      "Epoch: 19 Batch: 200, loss: 0.06341519136934241\n",
      "Epoch: 19 Batch: 300, loss: 1.369426409092221\n",
      "Epoch: 19 Batch: 400, loss: 0.9735118074865284\n",
      "Epoch: 19 Batch: 500, loss: 0.21344316635913055\n",
      "Epoch: 20 Batch: 0, loss: 1.2751002706661172\n",
      "Epoch: 20 Batch: 100, loss: 1.2667764355501818\n",
      "Epoch: 20 Batch: 200, loss: 2.091442892686655\n",
      "Epoch: 20 Batch: 300, loss: 0.07363637313055249\n",
      "Epoch: 20 Batch: 400, loss: 0.10382664644207901\n",
      "Epoch: 20 Batch: 500, loss: 0.5103309893761853\n",
      "Epoch: 21 Batch: 0, loss: 1.8000728902976455\n",
      "Epoch: 21 Batch: 100, loss: 1.0998961912313767\n",
      "Epoch: 21 Batch: 200, loss: 1.7171489368648283\n",
      "Epoch: 21 Batch: 300, loss: 0.9561685621660215\n",
      "Epoch: 21 Batch: 400, loss: 0.5813985539010473\n",
      "Epoch: 21 Batch: 500, loss: 0.9715815341417723\n",
      "Epoch: 22 Batch: 0, loss: 1.1047028828931003\n",
      "Epoch: 22 Batch: 100, loss: 0.7768471078877921\n",
      "Epoch: 22 Batch: 200, loss: 0.7836307220056243\n",
      "Epoch: 22 Batch: 300, loss: 0.04775619805676123\n",
      "Epoch: 22 Batch: 400, loss: 0.6879729614047109\n",
      "Epoch: 22 Batch: 500, loss: 0.527988303473478\n",
      "Epoch: 23 Batch: 0, loss: 0.046096670042809644\n",
      "Epoch: 23 Batch: 100, loss: 1.3830468396085072\n",
      "Epoch: 23 Batch: 200, loss: 0.10315424315343362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Batch: 300, loss: 0.08481319487731195\n",
      "Epoch: 23 Batch: 400, loss: 0.6559204116182972\n",
      "Epoch: 23 Batch: 500, loss: 1.0188856777416477\n",
      "Epoch: 24 Batch: 0, loss: 0.7451943506699132\n",
      "Epoch: 24 Batch: 100, loss: 0.08393137442095533\n",
      "Epoch: 24 Batch: 200, loss: 0.09354505293110424\n",
      "Epoch: 24 Batch: 300, loss: 0.15414070266618185\n",
      "Epoch: 24 Batch: 400, loss: 0.02074789060500051\n",
      "Epoch: 24 Batch: 500, loss: 0.9680075891226287\n",
      "Epoch: 25 Batch: 0, loss: 0.3432928318459553\n",
      "Epoch: 25 Batch: 100, loss: 1.4445875152359073\n",
      "Epoch: 25 Batch: 200, loss: 0.23359415239381398\n",
      "Epoch: 25 Batch: 300, loss: 0.6932307956482775\n",
      "Epoch: 25 Batch: 400, loss: 0.12760640997210898\n",
      "Epoch: 25 Batch: 500, loss: 0.24464230251399288\n",
      "Epoch: 26 Batch: 0, loss: 0.30325233389416756\n",
      "Epoch: 26 Batch: 100, loss: 0.1800223716650605\n",
      "Epoch: 26 Batch: 200, loss: 0.6025343460664814\n",
      "Epoch: 26 Batch: 300, loss: 0.27510692066058373\n",
      "Epoch: 26 Batch: 400, loss: 1.6311290778969851\n",
      "Epoch: 26 Batch: 500, loss: 0.21143680793515365\n",
      "Epoch: 27 Batch: 0, loss: 0.24096450185616353\n",
      "Epoch: 27 Batch: 100, loss: 0.0041422805435946735\n",
      "Epoch: 27 Batch: 200, loss: 0.11026043818284419\n",
      "Epoch: 27 Batch: 300, loss: 0.6361471953477729\n",
      "Epoch: 27 Batch: 400, loss: 0.08043917072476847\n",
      "Epoch: 27 Batch: 500, loss: 0.11217111222441056\n",
      "Epoch: 28 Batch: 0, loss: 0.2712840081127653\n",
      "Epoch: 28 Batch: 100, loss: 0.6423097930828584\n",
      "Epoch: 28 Batch: 200, loss: 0.31551502527719666\n",
      "Epoch: 28 Batch: 300, loss: 0.2864308084199323\n",
      "Epoch: 28 Batch: 400, loss: 1.047357943827867\n",
      "Epoch: 28 Batch: 500, loss: 0.43774144240942287\n",
      "Epoch: 29 Batch: 0, loss: 0.266305724530854\n",
      "Epoch: 29 Batch: 100, loss: 0.026300277442252627\n",
      "Epoch: 29 Batch: 200, loss: 0.016579331238990698\n",
      "Epoch: 29 Batch: 300, loss: 0.12144203632799462\n",
      "Epoch: 29 Batch: 400, loss: 0.4600885080552348\n",
      "Epoch: 29 Batch: 500, loss: 1.2589338465005937\n",
      "Epoch: 30 Batch: 0, loss: 0.05434972184231737\n",
      "Epoch: 30 Batch: 100, loss: 0.04736265199943402\n",
      "Epoch: 30 Batch: 200, loss: 0.5317370246932435\n",
      "Epoch: 30 Batch: 300, loss: 0.5258906477397268\n",
      "Epoch: 30 Batch: 400, loss: 0.16033552922611985\n",
      "Epoch: 30 Batch: 500, loss: 0.0009496739552200974\n",
      "Epoch: 31 Batch: 0, loss: 0.07028540091834051\n",
      "Epoch: 31 Batch: 100, loss: 1.6264461396773944\n",
      "Epoch: 31 Batch: 200, loss: 0.3710378604890579\n",
      "Epoch: 31 Batch: 300, loss: 0.43669343765112417\n",
      "Epoch: 31 Batch: 400, loss: 0.14014596405303575\n",
      "Epoch: 31 Batch: 500, loss: 0.34815233954238023\n",
      "Epoch: 32 Batch: 0, loss: 0.7015569424682288\n",
      "Epoch: 32 Batch: 100, loss: 0.22434515597834678\n",
      "Epoch: 32 Batch: 200, loss: 0.0934232391285042\n",
      "Epoch: 32 Batch: 300, loss: 0.8247253742897469\n",
      "Epoch: 32 Batch: 400, loss: 0.0374210819926235\n",
      "Epoch: 32 Batch: 500, loss: 0.5091283070817453\n",
      "Epoch: 33 Batch: 0, loss: 0.1170574341538765\n",
      "Epoch: 33 Batch: 100, loss: 5.84278289446465\n",
      "Epoch: 33 Batch: 200, loss: 0.0025070978796106025\n",
      "Epoch: 33 Batch: 300, loss: 0.22917751589844765\n",
      "Epoch: 33 Batch: 400, loss: 0.3127640374868757\n",
      "Epoch: 33 Batch: 500, loss: 0.31042980503115797\n",
      "Epoch: 34 Batch: 0, loss: 0.8683058952630537\n",
      "Epoch: 34 Batch: 100, loss: 1.744215338138223\n",
      "Epoch: 34 Batch: 200, loss: 0.03263060055559662\n",
      "Epoch: 34 Batch: 300, loss: 0.3055913491369969\n",
      "Epoch: 34 Batch: 400, loss: 0.15608173919228813\n",
      "Epoch: 34 Batch: 500, loss: 0.0017572910418754018\n",
      "Epoch: 35 Batch: 0, loss: 0.10588369807007184\n",
      "Epoch: 35 Batch: 100, loss: 0.042103490828820124\n",
      "Epoch: 35 Batch: 200, loss: 0.20417777960358974\n",
      "Epoch: 35 Batch: 300, loss: 0.0004966229513620362\n",
      "Epoch: 35 Batch: 400, loss: 0.3049512639097426\n",
      "Epoch: 35 Batch: 500, loss: 0.5414245459997429\n",
      "Epoch: 36 Batch: 0, loss: 0.3870077104960445\n",
      "Epoch: 36 Batch: 100, loss: 0.21002068020887849\n",
      "Epoch: 36 Batch: 200, loss: 0.00840211505456798\n",
      "Epoch: 36 Batch: 300, loss: 0.26508607727763067\n",
      "Epoch: 36 Batch: 400, loss: 1.8394087143464044\n",
      "Epoch: 36 Batch: 500, loss: 1.2214662978691229\n",
      "Epoch: 37 Batch: 0, loss: 0.21846801697459795\n",
      "Epoch: 37 Batch: 100, loss: 0.9315888403350391\n",
      "Epoch: 37 Batch: 200, loss: 0.009555837171077816\n",
      "Epoch: 37 Batch: 300, loss: 0.03570403777410654\n",
      "Epoch: 37 Batch: 400, loss: 0.45096524787983466\n",
      "Epoch: 37 Batch: 500, loss: 0.17433195769861418\n",
      "Epoch: 38 Batch: 0, loss: 0.37360922345801684\n",
      "Epoch: 38 Batch: 100, loss: 0.3504262437683832\n",
      "Epoch: 38 Batch: 200, loss: 0.2523435844859356\n",
      "Epoch: 38 Batch: 300, loss: 0.02863082812954305\n",
      "Epoch: 38 Batch: 400, loss: 0.7197897149607451\n",
      "Epoch: 38 Batch: 500, loss: 1.0885901728479914\n",
      "Epoch: 39 Batch: 0, loss: 0.10347979402917026\n",
      "Epoch: 39 Batch: 100, loss: 0.3263313176489372\n",
      "Epoch: 39 Batch: 200, loss: 0.01648841157122911\n",
      "Epoch: 39 Batch: 300, loss: 0.12414112583844919\n",
      "Epoch: 39 Batch: 400, loss: 0.265430165479081\n",
      "Epoch: 39 Batch: 500, loss: 0.7417344014241953\n",
      "Epoch: 40 Batch: 0, loss: 0.1333605193946098\n",
      "Epoch: 40 Batch: 100, loss: 1.990901819304744\n",
      "Epoch: 40 Batch: 200, loss: 0.26569967233752895\n",
      "Epoch: 40 Batch: 300, loss: 0.009534127423168713\n",
      "Epoch: 40 Batch: 400, loss: 0.029116352528786273\n",
      "Epoch: 40 Batch: 500, loss: 0.2746673633270308\n",
      "Epoch: 41 Batch: 0, loss: 0.0006607082973806895\n",
      "Epoch: 41 Batch: 100, loss: 0.4450693222097001\n",
      "Epoch: 41 Batch: 200, loss: 0.7322657965727694\n",
      "Epoch: 41 Batch: 300, loss: 0.2993505443616088\n",
      "Epoch: 41 Batch: 400, loss: 0.3464009732590584\n",
      "Epoch: 41 Batch: 500, loss: 0.0077981966968540106\n",
      "Epoch: 42 Batch: 0, loss: 0.35489970160802814\n",
      "Epoch: 42 Batch: 100, loss: 0.27044528057937484\n",
      "Epoch: 42 Batch: 200, loss: 9.279564841820037e-05\n",
      "Epoch: 42 Batch: 300, loss: 0.38608910163999094\n",
      "Epoch: 42 Batch: 400, loss: 0.1583718089176594\n",
      "Epoch: 42 Batch: 500, loss: 0.3077771761865112\n",
      "Epoch: 43 Batch: 0, loss: 0.27651199856910713\n",
      "Epoch: 43 Batch: 100, loss: 0.24475613316794703\n",
      "Epoch: 43 Batch: 200, loss: 0.13741786349085042\n",
      "Epoch: 43 Batch: 300, loss: 0.054626205598794546\n",
      "Epoch: 43 Batch: 400, loss: 0.10790603739806648\n",
      "Epoch: 43 Batch: 500, loss: 0.20309612370104263\n",
      "Epoch: 44 Batch: 0, loss: 0.29992981232086185\n",
      "Epoch: 44 Batch: 100, loss: 0.5726811310479746\n",
      "Epoch: 44 Batch: 200, loss: 0.04611131502275947\n",
      "Epoch: 44 Batch: 300, loss: 0.4348459026275838\n",
      "Epoch: 44 Batch: 400, loss: 0.43525023272185265\n",
      "Epoch: 44 Batch: 500, loss: 0.15405119064821216\n",
      "Epoch: 45 Batch: 0, loss: 0.14758569896528734\n",
      "Epoch: 45 Batch: 100, loss: 0.07652736380188777\n",
      "Epoch: 45 Batch: 200, loss: 0.11619034697465044\n",
      "Epoch: 45 Batch: 300, loss: 0.0004608635710864922\n",
      "Epoch: 45 Batch: 400, loss: 0.09207056896213225\n",
      "Epoch: 45 Batch: 500, loss: 0.0024988514816839803\n",
      "Epoch: 46 Batch: 0, loss: 0.3881443740053451\n",
      "Epoch: 46 Batch: 100, loss: 0.2749448597529356\n",
      "Epoch: 46 Batch: 200, loss: 1.1569829151707327\n",
      "Epoch: 46 Batch: 300, loss: 0.04502620138332637\n",
      "Epoch: 46 Batch: 400, loss: 0.11110905682978012\n",
      "Epoch: 46 Batch: 500, loss: 0.2580772184987016\n",
      "Epoch: 47 Batch: 0, loss: 0.0036891833950075\n",
      "Epoch: 47 Batch: 100, loss: 1.1257217827568151\n",
      "Epoch: 47 Batch: 200, loss: 0.3986741301711143\n",
      "Epoch: 47 Batch: 300, loss: 0.00017254232341148496\n",
      "Epoch: 47 Batch: 400, loss: 1.732916606352257\n",
      "Epoch: 47 Batch: 500, loss: 0.00019704282092939198\n",
      "Epoch: 48 Batch: 0, loss: 1.095735995089804\n",
      "Epoch: 48 Batch: 100, loss: 1.4244687174497144\n",
      "Epoch: 48 Batch: 200, loss: 0.014851440447514946\n",
      "Epoch: 48 Batch: 300, loss: 0.01589829137902703\n",
      "Epoch: 48 Batch: 400, loss: 0.022150289325561765\n",
      "Epoch: 48 Batch: 500, loss: 0.22724143958443363\n",
      "Epoch: 49 Batch: 0, loss: 0.5434559804900588\n",
      "Epoch: 49 Batch: 100, loss: 0.578177121636455\n",
      "Epoch: 49 Batch: 200, loss: 0.08929205162466071\n",
      "Epoch: 49 Batch: 300, loss: 0.2520524081820918\n",
      "Epoch: 49 Batch: 400, loss: 2.1554493375233195\n",
      "Epoch: 49 Batch: 500, loss: 0.10765452328745419\n",
      "Epoch: 50 Batch: 0, loss: 0.39364872719012833\n",
      "Epoch: 50 Batch: 100, loss: 0.2361096517600336\n",
      "Epoch: 50 Batch: 200, loss: 0.2913809712067382\n",
      "Epoch: 50 Batch: 300, loss: 0.40763645973983076\n",
      "Epoch: 50 Batch: 400, loss: 0.16669121164874937\n",
      "Epoch: 50 Batch: 500, loss: 0.0986679117947526\n",
      "Epoch: 51 Batch: 0, loss: 0.07428467506281676\n",
      "Epoch: 51 Batch: 100, loss: 0.33539581663586215\n",
      "Epoch: 51 Batch: 200, loss: 0.7745916038924897\n",
      "Epoch: 51 Batch: 300, loss: 0.17407375321937973\n",
      "Epoch: 51 Batch: 400, loss: 0.005346282623740483\n",
      "Epoch: 51 Batch: 500, loss: 0.1360061345593266\n",
      "Epoch: 52 Batch: 0, loss: 0.5074715450666766\n",
      "Epoch: 52 Batch: 100, loss: 0.9864789908827224\n",
      "Epoch: 52 Batch: 200, loss: 0.12576239255145216\n",
      "Epoch: 52 Batch: 300, loss: 0.8918327125250096\n",
      "Epoch: 52 Batch: 400, loss: 0.19159120671822522\n",
      "Epoch: 52 Batch: 500, loss: 0.3943756431585096\n",
      "Epoch: 53 Batch: 0, loss: 0.09796155051890534\n",
      "Epoch: 53 Batch: 100, loss: 0.07046058921110485\n",
      "Epoch: 53 Batch: 200, loss: 0.0050984768689585295\n",
      "Epoch: 53 Batch: 300, loss: 0.18816618296007137\n",
      "Epoch: 53 Batch: 400, loss: 0.80036755256667\n",
      "Epoch: 53 Batch: 500, loss: 0.9927014204706184\n",
      "Epoch: 54 Batch: 0, loss: 0.6096343685748583\n",
      "Epoch: 54 Batch: 100, loss: 0.2515714859331795\n",
      "Epoch: 54 Batch: 200, loss: 0.21084870017194104\n",
      "Epoch: 54 Batch: 300, loss: 0.049452271711209125\n",
      "Epoch: 54 Batch: 400, loss: 0.3619161511165117\n",
      "Epoch: 54 Batch: 500, loss: 0.310476089254272\n",
      "Epoch: 55 Batch: 0, loss: 0.1918181134456553\n",
      "Epoch: 55 Batch: 100, loss: 0.254566608691063\n",
      "Epoch: 55 Batch: 200, loss: 0.20022626655109574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 Batch: 300, loss: 0.001498892333924483\n",
      "Epoch: 55 Batch: 400, loss: 0.018937514212736085\n",
      "Epoch: 55 Batch: 500, loss: 0.8064896788338125\n",
      "Epoch: 56 Batch: 0, loss: 0.3101263460664965\n",
      "Epoch: 56 Batch: 100, loss: 0.03007202496533159\n",
      "Epoch: 56 Batch: 200, loss: 0.2557050223378197\n",
      "Epoch: 56 Batch: 300, loss: 0.07510274393437927\n",
      "Epoch: 56 Batch: 400, loss: 0.0017844280567933652\n",
      "Epoch: 56 Batch: 500, loss: 1.0752580484531555\n",
      "Epoch: 57 Batch: 0, loss: 0.007546154598895173\n",
      "Epoch: 57 Batch: 100, loss: 0.007460251241314447\n",
      "Epoch: 57 Batch: 200, loss: 0.5187829089707059\n",
      "Epoch: 57 Batch: 300, loss: 2.034949595404852\n",
      "Epoch: 57 Batch: 400, loss: 0.3190187666843871\n",
      "Epoch: 57 Batch: 500, loss: 0.3181826377046034\n",
      "Epoch: 58 Batch: 0, loss: 0.08803793518303288\n",
      "Epoch: 58 Batch: 100, loss: 0.04038209519805335\n",
      "Epoch: 58 Batch: 200, loss: 0.012543375046399215\n",
      "Epoch: 58 Batch: 300, loss: 0.13024346373977688\n",
      "Epoch: 58 Batch: 400, loss: 0.04630147953804212\n",
      "Epoch: 58 Batch: 500, loss: 0.8216705577421665\n",
      "Epoch: 59 Batch: 0, loss: 0.11711257000745383\n",
      "Epoch: 59 Batch: 100, loss: 0.004625327962275111\n",
      "Epoch: 59 Batch: 200, loss: 0.16998319321627042\n",
      "Epoch: 59 Batch: 300, loss: 0.23873502590246878\n",
      "Epoch: 59 Batch: 400, loss: 0.5265165668223586\n",
      "Epoch: 59 Batch: 500, loss: 0.012298301942388441\n",
      "Epoch: 60 Batch: 0, loss: 0.324524030180831\n",
      "Epoch: 60 Batch: 100, loss: 0.41255164014367207\n",
      "Epoch: 60 Batch: 200, loss: 0.6967447243892869\n",
      "Epoch: 60 Batch: 300, loss: 0.21863628883239605\n",
      "Epoch: 60 Batch: 400, loss: 0.5177394132229863\n",
      "Epoch: 60 Batch: 500, loss: 0.06861688096069986\n",
      "Epoch: 61 Batch: 0, loss: 0.9711988350648055\n",
      "Epoch: 61 Batch: 100, loss: 0.3207557612677075\n",
      "Epoch: 61 Batch: 200, loss: 0.09018652276673\n",
      "Epoch: 61 Batch: 300, loss: 0.105689842565126\n",
      "Epoch: 61 Batch: 400, loss: 0.3821065166620668\n",
      "Epoch: 61 Batch: 500, loss: 0.030004943328419346\n",
      "Epoch: 62 Batch: 0, loss: 0.005129562381729356\n",
      "Epoch: 62 Batch: 100, loss: 0.01158576765994935\n",
      "Epoch: 62 Batch: 200, loss: 0.5806309187606898\n",
      "Epoch: 62 Batch: 300, loss: 0.061968270141765286\n",
      "Epoch: 62 Batch: 400, loss: 0.11722449412322097\n",
      "Epoch: 62 Batch: 500, loss: 0.1431717787482204\n",
      "Epoch: 63 Batch: 0, loss: 1.3095675124016715\n",
      "Epoch: 63 Batch: 100, loss: 0.337399025433981\n",
      "Epoch: 63 Batch: 200, loss: 0.5455978370787092\n",
      "Epoch: 63 Batch: 300, loss: 0.017630483976537985\n",
      "Epoch: 63 Batch: 400, loss: 0.08907065373033705\n",
      "Epoch: 63 Batch: 500, loss: 0.1020880318040028\n",
      "Epoch: 64 Batch: 0, loss: 0.8942136603131973\n",
      "Epoch: 64 Batch: 100, loss: 0.0006158036288590505\n",
      "Epoch: 64 Batch: 200, loss: 0.31548122192616834\n",
      "Epoch: 64 Batch: 300, loss: 0.36170577249982994\n",
      "Epoch: 64 Batch: 400, loss: 0.14035789701891793\n",
      "Epoch: 64 Batch: 500, loss: 0.2586159100010461\n",
      "Epoch: 65 Batch: 0, loss: 0.9724843057492343\n",
      "Epoch: 65 Batch: 100, loss: 0.9236949234851679\n",
      "Epoch: 65 Batch: 200, loss: 0.5929996810784933\n",
      "Epoch: 65 Batch: 300, loss: 1.057552963501246\n",
      "Epoch: 65 Batch: 400, loss: 0.29137597642109747\n",
      "Epoch: 65 Batch: 500, loss: 0.25958517201600156\n",
      "Epoch: 66 Batch: 0, loss: 0.4742702295873432\n",
      "Epoch: 66 Batch: 100, loss: 0.026131689132018212\n",
      "Epoch: 66 Batch: 200, loss: 0.03332009098062168\n",
      "Epoch: 66 Batch: 300, loss: 0.43709382850993345\n",
      "Epoch: 66 Batch: 400, loss: 0.036367802307585\n",
      "Epoch: 66 Batch: 500, loss: 0.25701834045687777\n",
      "Epoch: 67 Batch: 0, loss: 0.0014851737897572756\n",
      "Epoch: 67 Batch: 100, loss: 1.2299464863982223\n",
      "Epoch: 67 Batch: 200, loss: 1.5266833968535445\n",
      "Epoch: 67 Batch: 300, loss: 0.03849581314224419\n",
      "Epoch: 67 Batch: 400, loss: 0.17789788823665642\n",
      "Epoch: 67 Batch: 500, loss: 0.0394887663131864\n",
      "Epoch: 68 Batch: 0, loss: 0.09419260505507987\n",
      "Epoch: 68 Batch: 100, loss: 0.02616782412173171\n",
      "Epoch: 68 Batch: 200, loss: 0.04271157875874743\n",
      "Epoch: 68 Batch: 300, loss: 0.8757253303800596\n",
      "Epoch: 68 Batch: 400, loss: 0.13279549403039612\n",
      "Epoch: 68 Batch: 500, loss: 1.9945085246445207\n",
      "Epoch: 69 Batch: 0, loss: 2.046517808446505\n",
      "Epoch: 69 Batch: 100, loss: 0.061825929167676416\n",
      "Epoch: 69 Batch: 200, loss: 2.0515194276577073\n",
      "Epoch: 69 Batch: 300, loss: 0.6160193919707757\n",
      "Epoch: 69 Batch: 400, loss: 0.050719965628739136\n",
      "Epoch: 69 Batch: 500, loss: 0.045182102504434885\n",
      "Epoch: 70 Batch: 0, loss: 0.040206804051947646\n",
      "Epoch: 70 Batch: 100, loss: 0.15087499958391254\n",
      "Epoch: 70 Batch: 200, loss: 0.2526464502798824\n",
      "Epoch: 70 Batch: 300, loss: 0.7995708651431318\n",
      "Epoch: 70 Batch: 400, loss: 0.02828443562710952\n",
      "Epoch: 70 Batch: 500, loss: 0.2762965690421452\n",
      "Epoch: 71 Batch: 0, loss: 0.39077556278503567\n",
      "Epoch: 71 Batch: 100, loss: 0.07023213939862842\n",
      "Epoch: 71 Batch: 200, loss: 0.4774049486099332\n",
      "Epoch: 71 Batch: 300, loss: 0.5577362672324435\n",
      "Epoch: 71 Batch: 400, loss: 0.40393079364749\n",
      "Epoch: 71 Batch: 500, loss: 0.20915041247545538\n",
      "Epoch: 72 Batch: 0, loss: 0.3250987238300675\n",
      "Epoch: 72 Batch: 100, loss: 0.18017124185152114\n",
      "Epoch: 72 Batch: 200, loss: 0.4373640323887717\n",
      "Epoch: 72 Batch: 300, loss: 0.013338017779789085\n",
      "Epoch: 72 Batch: 400, loss: 0.3627410341037813\n",
      "Epoch: 72 Batch: 500, loss: 0.003064248614361976\n",
      "Epoch: 73 Batch: 0, loss: 0.1563607741801172\n",
      "Epoch: 73 Batch: 100, loss: 1.8454603046817608\n",
      "Epoch: 73 Batch: 200, loss: 0.008817994294708899\n",
      "Epoch: 73 Batch: 300, loss: 0.27706330820657527\n",
      "Epoch: 73 Batch: 400, loss: 0.051197214568720055\n",
      "Epoch: 73 Batch: 500, loss: 0.3181755519903872\n",
      "Epoch: 74 Batch: 0, loss: 0.42122459055970257\n",
      "Epoch: 74 Batch: 100, loss: 0.40836967840853206\n",
      "Epoch: 74 Batch: 200, loss: 2.0705631320119773\n",
      "Epoch: 74 Batch: 300, loss: 0.06565447875135168\n",
      "Epoch: 74 Batch: 400, loss: 0.6735809639749346\n",
      "Epoch: 74 Batch: 500, loss: 0.7648746751257455\n",
      "Epoch: 75 Batch: 0, loss: 0.23245185553717676\n",
      "Epoch: 75 Batch: 100, loss: 0.8804823366566151\n",
      "Epoch: 75 Batch: 200, loss: 0.03210901144244556\n",
      "Epoch: 75 Batch: 300, loss: 0.1170866495703493\n",
      "Epoch: 75 Batch: 400, loss: 0.25363694297887585\n",
      "Epoch: 75 Batch: 500, loss: 0.156378655985766\n",
      "Epoch: 76 Batch: 0, loss: 0.18704967465740893\n",
      "Epoch: 76 Batch: 100, loss: 0.22944274219201255\n",
      "Epoch: 76 Batch: 200, loss: 0.666095927766761\n",
      "Epoch: 76 Batch: 300, loss: 0.08317736472726998\n",
      "Epoch: 76 Batch: 400, loss: 0.9669636846859205\n",
      "Epoch: 76 Batch: 500, loss: 1.1539582687065977\n",
      "Epoch: 77 Batch: 0, loss: 0.03463269690514495\n",
      "Epoch: 77 Batch: 100, loss: 0.2509634080343472\n",
      "Epoch: 77 Batch: 200, loss: 0.09702364289938072\n",
      "Epoch: 77 Batch: 300, loss: 0.0410619535728506\n",
      "Epoch: 77 Batch: 400, loss: 0.9077894459261426\n",
      "Epoch: 77 Batch: 500, loss: 0.09620912458784826\n",
      "Epoch: 78 Batch: 0, loss: 0.8638351390266517\n",
      "Epoch: 78 Batch: 100, loss: 0.5032571958468517\n",
      "Epoch: 78 Batch: 200, loss: 0.019117805514277195\n",
      "Epoch: 78 Batch: 300, loss: 0.9114711835901661\n",
      "Epoch: 78 Batch: 400, loss: 0.048101657091390485\n",
      "Epoch: 78 Batch: 500, loss: 0.19857892470233676\n",
      "Epoch: 79 Batch: 0, loss: 0.3034194785257254\n",
      "Epoch: 79 Batch: 100, loss: 0.11627074690383234\n",
      "Epoch: 79 Batch: 200, loss: 0.008282146806354453\n",
      "Epoch: 79 Batch: 300, loss: 0.028285660398470802\n",
      "Epoch: 79 Batch: 400, loss: 0.0003975813785913726\n",
      "Epoch: 79 Batch: 500, loss: 0.5077070119722009\n",
      "Epoch: 80 Batch: 0, loss: 0.0015675389228838991\n",
      "Epoch: 80 Batch: 100, loss: 0.22925229793975044\n",
      "Epoch: 80 Batch: 200, loss: 0.3844282053919384\n",
      "Epoch: 80 Batch: 300, loss: 0.3609899592154121\n",
      "Epoch: 80 Batch: 400, loss: 0.22791288822368921\n",
      "Epoch: 80 Batch: 500, loss: 0.18946682037454426\n",
      "Epoch: 81 Batch: 0, loss: 1.5199789335679963\n",
      "Epoch: 81 Batch: 100, loss: 0.048065231704276526\n",
      "Epoch: 81 Batch: 200, loss: 0.5005982383366561\n",
      "Epoch: 81 Batch: 300, loss: 0.09244815243281451\n",
      "Epoch: 81 Batch: 400, loss: 0.529980547521491\n",
      "Epoch: 81 Batch: 500, loss: 0.35871203837396454\n",
      "Epoch: 82 Batch: 0, loss: 0.01654825897744212\n",
      "Epoch: 82 Batch: 100, loss: 0.3562992807912998\n",
      "Epoch: 82 Batch: 200, loss: 0.5220670395054696\n",
      "Epoch: 82 Batch: 300, loss: 0.34227086987498\n",
      "Epoch: 82 Batch: 400, loss: 0.2343290221464863\n",
      "Epoch: 82 Batch: 500, loss: 0.4326907080310568\n",
      "Epoch: 83 Batch: 0, loss: 0.01613694142326417\n",
      "Epoch: 83 Batch: 100, loss: 0.06385604689899414\n",
      "Epoch: 83 Batch: 200, loss: 0.026239610181870075\n",
      "Epoch: 83 Batch: 300, loss: 0.12550922859538385\n",
      "Epoch: 83 Batch: 400, loss: 0.11148419634981041\n",
      "Epoch: 83 Batch: 500, loss: 0.015941519460489346\n",
      "Epoch: 84 Batch: 0, loss: 0.38044137700025604\n",
      "Epoch: 84 Batch: 100, loss: 0.061307101868956444\n",
      "Epoch: 84 Batch: 200, loss: 0.9262598873218264\n",
      "Epoch: 84 Batch: 300, loss: 0.0924068505330841\n",
      "Epoch: 84 Batch: 400, loss: 0.11436968429342788\n",
      "Epoch: 84 Batch: 500, loss: 0.40149749437346305\n",
      "Epoch: 85 Batch: 0, loss: 0.1870599916505214\n",
      "Epoch: 85 Batch: 100, loss: 0.2958175736623184\n",
      "Epoch: 85 Batch: 200, loss: 0.251808060621079\n",
      "Epoch: 85 Batch: 300, loss: 0.5306967320689278\n",
      "Epoch: 85 Batch: 400, loss: 0.0011331760086206782\n",
      "Epoch: 85 Batch: 500, loss: 0.03660039085329478\n",
      "Epoch: 86 Batch: 0, loss: 0.13110445565209605\n",
      "Epoch: 86 Batch: 100, loss: 3.6036428045086897\n",
      "Epoch: 86 Batch: 200, loss: 0.24906100890815647\n",
      "Epoch: 86 Batch: 300, loss: 0.2110057669078159\n",
      "Epoch: 86 Batch: 400, loss: 0.006281009472126714\n",
      "Epoch: 86 Batch: 500, loss: 0.07174163066700205\n",
      "Epoch: 87 Batch: 0, loss: 0.33880296219128475\n",
      "Epoch: 87 Batch: 100, loss: 0.0005515285435814515\n",
      "Epoch: 87 Batch: 200, loss: 0.5183311025347059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 Batch: 300, loss: 0.5935190849270788\n",
      "Epoch: 87 Batch: 400, loss: 0.10556049620702757\n",
      "Epoch: 87 Batch: 500, loss: 0.00568288084100524\n",
      "Epoch: 88 Batch: 0, loss: 0.12792586918310622\n",
      "Epoch: 88 Batch: 100, loss: 0.3795170118375526\n",
      "Epoch: 88 Batch: 200, loss: 0.43945632579680727\n",
      "Epoch: 88 Batch: 300, loss: 3.846248887878397e-05\n",
      "Epoch: 88 Batch: 400, loss: 0.10680395753567978\n",
      "Epoch: 88 Batch: 500, loss: 0.7315493485875699\n",
      "Epoch: 89 Batch: 0, loss: 0.10656745508900135\n",
      "Epoch: 89 Batch: 100, loss: 0.18433404523894908\n",
      "Epoch: 89 Batch: 200, loss: 0.1444779343032526\n",
      "Epoch: 89 Batch: 300, loss: 0.09479506961015435\n",
      "Epoch: 89 Batch: 400, loss: 0.4377269126556663\n",
      "Epoch: 89 Batch: 500, loss: 0.883171546108192\n",
      "Epoch: 90 Batch: 0, loss: 0.4579019917758165\n",
      "Epoch: 90 Batch: 100, loss: 0.32346654220194887\n",
      "Epoch: 90 Batch: 200, loss: 0.08266284618541826\n",
      "Epoch: 90 Batch: 300, loss: 0.3971377044405933\n",
      "Epoch: 90 Batch: 400, loss: 0.9934478510106487\n",
      "Epoch: 90 Batch: 500, loss: 0.30576164651244064\n",
      "Epoch: 91 Batch: 0, loss: 0.16356429647058762\n",
      "Epoch: 91 Batch: 100, loss: 0.02633311986266605\n",
      "Epoch: 91 Batch: 200, loss: 0.24790733402207324\n",
      "Epoch: 91 Batch: 300, loss: 0.4155388694968694\n",
      "Epoch: 91 Batch: 400, loss: 0.099857805094248\n",
      "Epoch: 91 Batch: 500, loss: 0.3381258922472086\n",
      "Epoch: 92 Batch: 0, loss: 0.7087285233634233\n",
      "Epoch: 92 Batch: 100, loss: 0.005765203383311229\n",
      "Epoch: 92 Batch: 200, loss: 0.17360673239339733\n",
      "Epoch: 92 Batch: 300, loss: 0.09051349873517939\n",
      "Epoch: 92 Batch: 400, loss: 0.36013677833357033\n",
      "Epoch: 92 Batch: 500, loss: 0.0469378113034275\n",
      "Epoch: 93 Batch: 0, loss: 0.2768498900154414\n",
      "Epoch: 93 Batch: 100, loss: 0.08372642931556275\n",
      "Epoch: 93 Batch: 200, loss: 0.03829134525804307\n",
      "Epoch: 93 Batch: 300, loss: 0.09147411112682262\n",
      "Epoch: 93 Batch: 400, loss: 0.010030792532355219\n",
      "Epoch: 93 Batch: 500, loss: 0.32248419646146315\n",
      "Epoch: 94 Batch: 0, loss: 0.009729520264477272\n",
      "Epoch: 94 Batch: 100, loss: 0.0011702294551765653\n",
      "Epoch: 94 Batch: 200, loss: 0.24618880704635906\n",
      "Epoch: 94 Batch: 300, loss: 1.1289008985919764\n",
      "Epoch: 94 Batch: 400, loss: 0.03789589269691131\n",
      "Epoch: 94 Batch: 500, loss: 0.45852852819177664\n",
      "Epoch: 95 Batch: 0, loss: 0.0016567932061534762\n",
      "Epoch: 95 Batch: 100, loss: 0.12437939535409573\n",
      "Epoch: 95 Batch: 200, loss: 0.05143811729031266\n",
      "Epoch: 95 Batch: 300, loss: 0.1060541839440269\n",
      "Epoch: 95 Batch: 400, loss: 0.08824876357163583\n",
      "Epoch: 95 Batch: 500, loss: 0.052108569118180785\n",
      "Epoch: 96 Batch: 0, loss: 0.5254059458666507\n",
      "Epoch: 96 Batch: 100, loss: 7.517104463852872\n",
      "Epoch: 96 Batch: 200, loss: 0.2264293111159093\n",
      "Epoch: 96 Batch: 300, loss: 0.2731838874764808\n",
      "Epoch: 96 Batch: 400, loss: 0.07847517086471051\n",
      "Epoch: 96 Batch: 500, loss: 0.06815989430405382\n",
      "Epoch: 97 Batch: 0, loss: 0.15678710433202867\n",
      "Epoch: 97 Batch: 100, loss: 1.1307795355850057\n",
      "Epoch: 97 Batch: 200, loss: 0.008797894259245596\n",
      "Epoch: 97 Batch: 300, loss: 0.04254727884874487\n",
      "Epoch: 97 Batch: 400, loss: 0.06815459147312167\n",
      "Epoch: 97 Batch: 500, loss: 0.962856695815463\n",
      "Epoch: 98 Batch: 0, loss: 0.005329446626460217\n",
      "Epoch: 98 Batch: 100, loss: 0.11261517217976158\n",
      "Epoch: 98 Batch: 200, loss: 0.5111525459337704\n",
      "Epoch: 98 Batch: 300, loss: 0.46681331377484536\n",
      "Epoch: 98 Batch: 400, loss: 0.16158415725178363\n",
      "Epoch: 98 Batch: 500, loss: 1.0648027342550725\n",
      "Epoch: 99 Batch: 0, loss: 0.20837486552119158\n",
      "Epoch: 99 Batch: 100, loss: 0.00018892440115294074\n",
      "Epoch: 99 Batch: 200, loss: 3.7317018825859924\n",
      "Epoch: 99 Batch: 300, loss: 0.18730567928517752\n",
      "Epoch: 99 Batch: 400, loss: 0.5411687868517746\n",
      "Epoch: 99 Batch: 500, loss: 0.06510821955211904\n",
      "Epoch: 100 Batch: 0, loss: 1.04202348414861\n",
      "Epoch: 100 Batch: 100, loss: 0.30274970153497205\n",
      "Epoch: 100 Batch: 200, loss: 2.1058893119860036\n",
      "Epoch: 100 Batch: 300, loss: 0.24075220701361041\n",
      "Epoch: 100 Batch: 400, loss: 0.02388048626786009\n",
      "Epoch: 100 Batch: 500, loss: 0.8888128539047037\n",
      "Epoch: 101 Batch: 0, loss: 0.069504373017711\n",
      "Epoch: 101 Batch: 100, loss: 0.8738541365749622\n",
      "Epoch: 101 Batch: 200, loss: 0.2497933243637235\n",
      "Epoch: 101 Batch: 300, loss: 0.2899254800005224\n",
      "Epoch: 101 Batch: 400, loss: 0.24648836785068956\n",
      "Epoch: 101 Batch: 500, loss: 0.09868031614187839\n",
      "Epoch: 102 Batch: 0, loss: 0.41777400172867435\n",
      "Epoch: 102 Batch: 100, loss: 0.08235368150539199\n",
      "Epoch: 102 Batch: 200, loss: 0.10514082147416336\n",
      "Epoch: 102 Batch: 300, loss: 0.20938517720391317\n",
      "Epoch: 102 Batch: 400, loss: 0.19333105883797322\n",
      "Epoch: 102 Batch: 500, loss: 0.02861772067554957\n",
      "Epoch: 103 Batch: 0, loss: 0.12731149251671492\n",
      "Epoch: 103 Batch: 100, loss: 0.011014810838258572\n",
      "Epoch: 103 Batch: 200, loss: 0.9362608724161948\n",
      "Epoch: 103 Batch: 300, loss: 0.11671366654214274\n",
      "Epoch: 103 Batch: 400, loss: 8.573622638619406e-05\n",
      "Epoch: 103 Batch: 500, loss: 0.08733374330133983\n",
      "Epoch: 104 Batch: 0, loss: 0.24537203195651744\n",
      "Epoch: 104 Batch: 100, loss: 0.007006961622500685\n",
      "Epoch: 104 Batch: 200, loss: 0.2426103356504833\n",
      "Epoch: 104 Batch: 300, loss: 0.28130100159713856\n",
      "Epoch: 104 Batch: 400, loss: 0.3803630807366877\n",
      "Epoch: 104 Batch: 500, loss: 0.057813460290890996\n",
      "Epoch: 105 Batch: 0, loss: 0.061526173093508\n",
      "Epoch: 105 Batch: 100, loss: 0.4170043594577515\n",
      "Epoch: 105 Batch: 200, loss: 0.5606071554278539\n",
      "Epoch: 105 Batch: 300, loss: 0.07341668070168388\n",
      "Epoch: 105 Batch: 400, loss: 0.4343826937843634\n",
      "Epoch: 105 Batch: 500, loss: 0.3653407655482954\n",
      "Epoch: 106 Batch: 0, loss: 0.3596024254942574\n",
      "Epoch: 106 Batch: 100, loss: 0.09220266957791838\n",
      "Epoch: 106 Batch: 200, loss: 0.06478743166746394\n",
      "Epoch: 106 Batch: 300, loss: 6.824657603805245e-05\n",
      "Epoch: 106 Batch: 400, loss: 0.028205958107722542\n",
      "Epoch: 106 Batch: 500, loss: 0.06574829498299131\n",
      "Epoch: 107 Batch: 0, loss: 0.10250794346359202\n",
      "Epoch: 107 Batch: 100, loss: 0.4205161897431052\n",
      "Epoch: 107 Batch: 200, loss: 0.8855461771910624\n",
      "Epoch: 107 Batch: 300, loss: 0.2924859821799857\n",
      "Epoch: 107 Batch: 400, loss: 0.17203498073897414\n",
      "Epoch: 107 Batch: 500, loss: 0.6681169805730786\n",
      "Epoch: 108 Batch: 0, loss: 1.1238586368244916\n",
      "Epoch: 108 Batch: 100, loss: 0.0263663995314709\n",
      "Epoch: 108 Batch: 200, loss: 0.08376784438213587\n",
      "Epoch: 108 Batch: 300, loss: 0.029451484095630285\n",
      "Epoch: 108 Batch: 400, loss: 0.3400651593790506\n",
      "Epoch: 108 Batch: 500, loss: 0.0014140127414952544\n",
      "Epoch: 109 Batch: 0, loss: 0.10358581925268968\n",
      "Epoch: 109 Batch: 100, loss: 0.025508273969240762\n",
      "Epoch: 109 Batch: 200, loss: 1.254973830639702\n",
      "Epoch: 109 Batch: 300, loss: 0.028157460444444064\n",
      "Epoch: 109 Batch: 400, loss: 0.15873034225889096\n",
      "Epoch: 109 Batch: 500, loss: 0.30387592271229397\n",
      "Epoch: 110 Batch: 0, loss: 0.05303130209355061\n",
      "Epoch: 110 Batch: 100, loss: 0.4888128278395639\n",
      "Epoch: 110 Batch: 200, loss: 0.15372051039205814\n",
      "Epoch: 110 Batch: 300, loss: 1.061846069067516\n",
      "Epoch: 110 Batch: 400, loss: 1.5549936658646322\n",
      "Epoch: 110 Batch: 500, loss: 0.18176098843495042\n",
      "Epoch: 111 Batch: 0, loss: 0.10843177083546682\n",
      "Epoch: 111 Batch: 100, loss: 0.2462872056750614\n",
      "Epoch: 111 Batch: 200, loss: 0.022781588534024892\n",
      "Epoch: 111 Batch: 300, loss: 0.0014851524161592921\n",
      "Epoch: 111 Batch: 400, loss: 0.0005129287717080485\n",
      "Epoch: 111 Batch: 500, loss: 0.1533491338427698\n",
      "Epoch: 112 Batch: 0, loss: 2.111721248652358\n",
      "Epoch: 112 Batch: 100, loss: 0.02332987063631553\n",
      "Epoch: 112 Batch: 200, loss: 0.11881960409685172\n",
      "Epoch: 112 Batch: 300, loss: 0.5657319663241512\n",
      "Epoch: 112 Batch: 400, loss: 0.1173268523610315\n",
      "Epoch: 112 Batch: 500, loss: 0.003563276237131359\n",
      "Epoch: 113 Batch: 0, loss: 0.012068537751636209\n",
      "Epoch: 113 Batch: 100, loss: 0.006463959295070613\n",
      "Epoch: 113 Batch: 200, loss: 0.2747654562738793\n",
      "Epoch: 113 Batch: 300, loss: 1.006855331760566\n",
      "Epoch: 113 Batch: 400, loss: 0.024063571559145906\n",
      "Epoch: 113 Batch: 500, loss: 0.05497348172010556\n",
      "Epoch: 114 Batch: 0, loss: 0.20070134082941976\n",
      "Epoch: 114 Batch: 100, loss: 0.06353831618946636\n",
      "Epoch: 114 Batch: 200, loss: 0.0813108870075031\n",
      "Epoch: 114 Batch: 300, loss: 0.00638275501984251\n",
      "Epoch: 114 Batch: 400, loss: 0.0248325020878562\n",
      "Epoch: 114 Batch: 500, loss: 0.43400119946046195\n",
      "Epoch: 115 Batch: 0, loss: 0.0008336328512876849\n",
      "Epoch: 115 Batch: 100, loss: 0.002255686667337531\n",
      "Epoch: 115 Batch: 200, loss: 0.2979218440419108\n",
      "Epoch: 115 Batch: 300, loss: 0.06410914309534763\n",
      "Epoch: 115 Batch: 400, loss: 0.0026189972742488503\n",
      "Epoch: 115 Batch: 500, loss: 0.10836554684819635\n",
      "Epoch: 116 Batch: 0, loss: 0.15855804672531906\n",
      "Epoch: 116 Batch: 100, loss: 0.1698302783191806\n",
      "Epoch: 116 Batch: 200, loss: 0.033963876233011565\n",
      "Epoch: 116 Batch: 300, loss: 0.012215916517508536\n",
      "Epoch: 116 Batch: 400, loss: 0.016313627397790603\n",
      "Epoch: 116 Batch: 500, loss: 0.10741097099497829\n",
      "Epoch: 117 Batch: 0, loss: 0.007836432567977504\n",
      "Epoch: 117 Batch: 100, loss: 0.014142317357569264\n",
      "Epoch: 117 Batch: 200, loss: 0.05371055013089581\n",
      "Epoch: 117 Batch: 300, loss: 0.11434652643037106\n",
      "Epoch: 117 Batch: 400, loss: 0.08305415932219848\n",
      "Epoch: 117 Batch: 500, loss: 0.003157931516668802\n",
      "Epoch: 118 Batch: 0, loss: 0.4672066904524087\n",
      "Epoch: 118 Batch: 100, loss: 7.93402380356446e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118 Batch: 200, loss: 0.4010115776777767\n",
      "Epoch: 118 Batch: 300, loss: 0.7028859088204736\n",
      "Epoch: 118 Batch: 400, loss: 0.24075951837825138\n",
      "Epoch: 118 Batch: 500, loss: 0.022191317376890833\n",
      "Epoch: 119 Batch: 0, loss: 1.583676782444218\n",
      "Epoch: 119 Batch: 100, loss: 0.00042303315571495635\n",
      "Epoch: 119 Batch: 200, loss: 0.08045688887561125\n",
      "Epoch: 119 Batch: 300, loss: 0.11742951673224467\n",
      "Epoch: 119 Batch: 400, loss: 0.024708916720929466\n",
      "Epoch: 119 Batch: 500, loss: 0.04259623953693813\n",
      "Epoch: 120 Batch: 0, loss: 0.11217268535364511\n",
      "Epoch: 120 Batch: 100, loss: 0.033063506921872814\n",
      "Epoch: 120 Batch: 200, loss: 0.6699537387150392\n",
      "Epoch: 120 Batch: 300, loss: 0.003972419506914452\n",
      "Epoch: 120 Batch: 400, loss: 2.1580140716425587\n",
      "Epoch: 120 Batch: 500, loss: 0.2212782674789385\n",
      "Epoch: 121 Batch: 0, loss: 0.14858474427642618\n",
      "Epoch: 121 Batch: 100, loss: 0.03738039869816903\n",
      "Epoch: 121 Batch: 200, loss: 0.5043122988021512\n",
      "Epoch: 121 Batch: 300, loss: 0.001071382447350763\n",
      "Epoch: 121 Batch: 400, loss: 0.2056892535742113\n",
      "Epoch: 121 Batch: 500, loss: 1.0850402134730786\n",
      "Epoch: 122 Batch: 0, loss: 0.24112670935141461\n",
      "Epoch: 122 Batch: 100, loss: 0.23340545861726528\n",
      "Epoch: 122 Batch: 200, loss: 0.3052161298665185\n",
      "Epoch: 122 Batch: 300, loss: 0.3467310027809054\n",
      "Epoch: 122 Batch: 400, loss: 0.43206450673713087\n",
      "Epoch: 122 Batch: 500, loss: 0.19596085206781985\n",
      "Epoch: 123 Batch: 0, loss: 0.5019386588098075\n",
      "Epoch: 123 Batch: 100, loss: 0.4811695010919823\n",
      "Epoch: 123 Batch: 200, loss: 0.46822121750726453\n",
      "Epoch: 123 Batch: 300, loss: 0.651152651104518\n",
      "Epoch: 123 Batch: 400, loss: 0.02474316747515073\n",
      "Epoch: 123 Batch: 500, loss: 0.5334167391346389\n",
      "Epoch: 124 Batch: 0, loss: 0.3367568628715021\n",
      "Epoch: 124 Batch: 100, loss: 0.5673052561349712\n",
      "Epoch: 124 Batch: 200, loss: 0.5513510512032163\n",
      "Epoch: 124 Batch: 300, loss: 0.6161096625279795\n",
      "Epoch: 124 Batch: 400, loss: 0.01428526538650538\n",
      "Epoch: 124 Batch: 500, loss: 0.2888289720858396\n",
      "Epoch: 125 Batch: 0, loss: 0.19785749285473916\n",
      "Epoch: 125 Batch: 100, loss: 0.024729222899052952\n",
      "Epoch: 125 Batch: 200, loss: 0.09947252433829494\n",
      "Epoch: 125 Batch: 300, loss: 0.015114659876185151\n",
      "Epoch: 125 Batch: 400, loss: 1.9397096276506032\n",
      "Epoch: 125 Batch: 500, loss: 0.3631384455486761\n",
      "Epoch: 126 Batch: 0, loss: 0.5452053342128194\n",
      "Epoch: 126 Batch: 100, loss: 0.4039477640697207\n",
      "Epoch: 126 Batch: 200, loss: 0.24139371581751035\n",
      "Epoch: 126 Batch: 300, loss: 0.0011638364342662641\n",
      "Epoch: 126 Batch: 400, loss: 0.49819740212098135\n",
      "Epoch: 126 Batch: 500, loss: 0.021384986912277117\n",
      "Epoch: 127 Batch: 0, loss: 0.13032305811214273\n",
      "Epoch: 127 Batch: 100, loss: 0.6035896651495\n",
      "Epoch: 127 Batch: 200, loss: 0.02112153831689777\n",
      "Epoch: 127 Batch: 300, loss: 0.16272209981995178\n",
      "Epoch: 127 Batch: 400, loss: 2.159242111491312\n",
      "Epoch: 127 Batch: 500, loss: 0.10421873580381519\n",
      "Epoch: 128 Batch: 0, loss: 0.1122367249821354\n",
      "Epoch: 128 Batch: 100, loss: 1.0906478743208607\n",
      "Epoch: 128 Batch: 200, loss: 0.022552831450693263\n",
      "Epoch: 128 Batch: 300, loss: 0.3623658259694499\n",
      "Epoch: 128 Batch: 400, loss: 0.9479599625403141\n",
      "Epoch: 128 Batch: 500, loss: 0.0719953369292303\n",
      "Epoch: 129 Batch: 0, loss: 0.00047883714145552207\n",
      "Epoch: 129 Batch: 100, loss: 0.027959231256603358\n",
      "Epoch: 129 Batch: 200, loss: 0.02303970705370879\n",
      "Epoch: 129 Batch: 300, loss: 1.076752919394572\n",
      "Epoch: 129 Batch: 400, loss: 0.2123498849377225\n",
      "Epoch: 129 Batch: 500, loss: 0.12691262392784108\n",
      "Epoch: 130 Batch: 0, loss: 0.598121709317878\n",
      "Epoch: 130 Batch: 100, loss: 0.3730795398269904\n",
      "Epoch: 130 Batch: 200, loss: 0.05879817779177328\n",
      "Epoch: 130 Batch: 300, loss: 0.09466479407962106\n",
      "Epoch: 130 Batch: 400, loss: 0.8487087261480103\n",
      "Epoch: 130 Batch: 500, loss: 0.4526590147166075\n",
      "Epoch: 131 Batch: 0, loss: 0.14397366145637233\n",
      "Epoch: 131 Batch: 100, loss: 0.8360746019942245\n",
      "Epoch: 131 Batch: 200, loss: 0.43573943637038726\n",
      "Epoch: 131 Batch: 300, loss: 1.228906606102111\n",
      "Epoch: 131 Batch: 400, loss: 0.35433787065159045\n",
      "Epoch: 131 Batch: 500, loss: 0.021454908646332475\n",
      "Epoch: 132 Batch: 0, loss: 0.05331527454742439\n",
      "Epoch: 132 Batch: 100, loss: 0.35712403789548797\n",
      "Epoch: 132 Batch: 200, loss: 0.04667327417140834\n",
      "Epoch: 132 Batch: 300, loss: 0.11288620780265542\n",
      "Epoch: 132 Batch: 400, loss: 0.0012670953476818924\n",
      "Epoch: 132 Batch: 500, loss: 0.1367264316661441\n",
      "Epoch: 133 Batch: 0, loss: 0.00046386057422695854\n",
      "Epoch: 133 Batch: 100, loss: 0.1938415545095654\n",
      "Epoch: 133 Batch: 200, loss: 0.0009510565004752436\n",
      "Epoch: 133 Batch: 300, loss: 0.004848917718136351\n",
      "Epoch: 133 Batch: 400, loss: 0.08063141904188074\n",
      "Epoch: 133 Batch: 500, loss: 0.2752433998350263\n",
      "Epoch: 134 Batch: 0, loss: 0.0581615301747784\n",
      "Epoch: 134 Batch: 100, loss: 0.048425821234473476\n",
      "Epoch: 134 Batch: 200, loss: 0.1402843675678787\n",
      "Epoch: 134 Batch: 300, loss: 0.31832856447530383\n",
      "Epoch: 134 Batch: 400, loss: 0.0045022571510874715\n",
      "Epoch: 134 Batch: 500, loss: 0.2381902463740453\n",
      "Epoch: 135 Batch: 0, loss: 0.8843671287711512\n",
      "Epoch: 135 Batch: 100, loss: 0.616004057266188\n",
      "Epoch: 135 Batch: 200, loss: 0.2978039588872415\n",
      "Epoch: 135 Batch: 300, loss: 0.0005854154448959298\n",
      "Epoch: 135 Batch: 400, loss: 0.09717438019299014\n",
      "Epoch: 135 Batch: 500, loss: 0.03946256384781047\n",
      "Epoch: 136 Batch: 0, loss: 0.9875416392718381\n",
      "Epoch: 136 Batch: 100, loss: 0.03921006533745214\n",
      "Epoch: 136 Batch: 200, loss: 0.23282336230432313\n",
      "Epoch: 136 Batch: 300, loss: 0.13750029401616307\n",
      "Epoch: 136 Batch: 400, loss: 0.08263066108659893\n",
      "Epoch: 136 Batch: 500, loss: 0.08062256581055571\n",
      "Epoch: 137 Batch: 0, loss: 0.3211750095974347\n",
      "Epoch: 137 Batch: 100, loss: 0.007330104136222181\n",
      "Epoch: 137 Batch: 200, loss: 0.28912239557437036\n",
      "Epoch: 137 Batch: 300, loss: 0.05760910020014624\n",
      "Epoch: 137 Batch: 400, loss: 0.1076249439894317\n",
      "Epoch: 137 Batch: 500, loss: 0.007115930514840135\n",
      "Epoch: 138 Batch: 0, loss: 1.1429078914840813\n",
      "Epoch: 138 Batch: 100, loss: 0.08522414156814453\n",
      "Epoch: 138 Batch: 200, loss: 0.20141298813739314\n",
      "Epoch: 138 Batch: 300, loss: 0.0009698884731751821\n",
      "Epoch: 138 Batch: 400, loss: 0.37041486715878724\n",
      "Epoch: 138 Batch: 500, loss: 0.17641122042171214\n",
      "Epoch: 139 Batch: 0, loss: 0.7616652455491976\n",
      "Epoch: 139 Batch: 100, loss: 0.1597945760331757\n",
      "Epoch: 139 Batch: 200, loss: 0.003639597100845098\n",
      "Epoch: 139 Batch: 300, loss: 0.10260278173172199\n",
      "Epoch: 139 Batch: 400, loss: 0.046474209568390254\n",
      "Epoch: 139 Batch: 500, loss: 0.0916233851941834\n",
      "Epoch: 140 Batch: 0, loss: 0.2623609808107752\n",
      "Epoch: 140 Batch: 100, loss: 0.42573471886530867\n",
      "Epoch: 140 Batch: 200, loss: 0.44737528955075095\n",
      "Epoch: 140 Batch: 300, loss: 0.1255623317742853\n",
      "Epoch: 140 Batch: 400, loss: 0.34357178191409565\n",
      "Epoch: 140 Batch: 500, loss: 0.03752237598155062\n",
      "Epoch: 141 Batch: 0, loss: 0.4995890829334371\n",
      "Epoch: 141 Batch: 100, loss: 0.004840017974344863\n",
      "Epoch: 141 Batch: 200, loss: 0.24172354633510176\n",
      "Epoch: 141 Batch: 300, loss: 0.41753425622984974\n",
      "Epoch: 141 Batch: 400, loss: 0.17401255061504858\n",
      "Epoch: 141 Batch: 500, loss: 0.4606694290147037\n",
      "Epoch: 142 Batch: 0, loss: 0.030215410447245475\n",
      "Epoch: 142 Batch: 100, loss: 0.0932743305782759\n",
      "Epoch: 142 Batch: 200, loss: 0.3562415405757444\n",
      "Epoch: 142 Batch: 300, loss: 0.00803781351287997\n",
      "Epoch: 142 Batch: 400, loss: 0.5759585511038403\n",
      "Epoch: 142 Batch: 500, loss: 0.17421156270761312\n",
      "Epoch: 143 Batch: 0, loss: 0.17327204985438358\n",
      "Epoch: 143 Batch: 100, loss: 0.009155152995266832\n",
      "Epoch: 143 Batch: 200, loss: 0.020205167754113317\n",
      "Epoch: 143 Batch: 300, loss: 0.907499030596994\n",
      "Epoch: 143 Batch: 400, loss: 0.39720626853780466\n",
      "Epoch: 143 Batch: 500, loss: 0.032702557923498334\n",
      "Epoch: 144 Batch: 0, loss: 0.2307692130484322\n",
      "Epoch: 144 Batch: 100, loss: 0.03836260450886525\n",
      "Epoch: 144 Batch: 200, loss: 0.49240859976176343\n",
      "Epoch: 144 Batch: 300, loss: 0.1163209076605094\n",
      "Epoch: 144 Batch: 400, loss: 1.5914558035791564\n",
      "Epoch: 144 Batch: 500, loss: 0.00816500234615917\n",
      "Epoch: 145 Batch: 0, loss: 0.1576519608928608\n",
      "Epoch: 145 Batch: 100, loss: 0.47606981348829547\n",
      "Epoch: 145 Batch: 200, loss: 0.00028809799087678564\n",
      "Epoch: 145 Batch: 300, loss: 0.5068203786068795\n",
      "Epoch: 145 Batch: 400, loss: 0.004824082683194153\n",
      "Epoch: 145 Batch: 500, loss: 0.2326098523178009\n",
      "Epoch: 146 Batch: 0, loss: 0.22824845346858022\n",
      "Epoch: 146 Batch: 100, loss: 0.5552160982951188\n",
      "Epoch: 146 Batch: 200, loss: 0.0036896830455333744\n",
      "Epoch: 146 Batch: 300, loss: 1.5832133027664168\n",
      "Epoch: 146 Batch: 400, loss: 0.13406479484732084\n",
      "Epoch: 146 Batch: 500, loss: 0.11424005069673646\n",
      "Epoch: 147 Batch: 0, loss: 0.16739477915236997\n",
      "Epoch: 147 Batch: 100, loss: 0.034516075954766856\n",
      "Epoch: 147 Batch: 200, loss: 0.012700328401989975\n",
      "Epoch: 147 Batch: 300, loss: 0.21122789730994046\n",
      "Epoch: 147 Batch: 400, loss: 0.14736011624218673\n",
      "Epoch: 147 Batch: 500, loss: 0.07983001400002249\n",
      "Epoch: 148 Batch: 0, loss: 0.8881301181983259\n",
      "Epoch: 148 Batch: 100, loss: 0.37979948560309945\n",
      "Epoch: 148 Batch: 200, loss: 0.4026202400233642\n",
      "Epoch: 148 Batch: 300, loss: 0.25539514979980565\n",
      "Epoch: 148 Batch: 400, loss: 0.09791514749308926\n",
      "Epoch: 148 Batch: 500, loss: 0.2089793365284036\n",
      "Epoch: 149 Batch: 0, loss: 1.1851623652301833\n",
      "Epoch: 149 Batch: 100, loss: 0.13171268182719761\n",
      "Epoch: 149 Batch: 200, loss: 0.20851190188423732\n",
      "Epoch: 149 Batch: 300, loss: 0.006461443492058623\n",
      "Epoch: 149 Batch: 400, loss: 0.5720398863465684\n",
      "Epoch: 149 Batch: 500, loss: 0.0602197424695513\n",
      "Epoch: 150 Batch: 0, loss: 0.5116822900589691\n",
      "Epoch: 150 Batch: 100, loss: 0.4980701686859395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150 Batch: 200, loss: 2.4275403036701633\n",
      "Epoch: 150 Batch: 300, loss: 1.9403602464811296\n",
      "Epoch: 150 Batch: 400, loss: 0.2317259901742204\n",
      "Epoch: 150 Batch: 500, loss: 0.09101622460346205\n",
      "Epoch: 151 Batch: 0, loss: 0.5715687005783104\n",
      "Epoch: 151 Batch: 100, loss: 2.0936712277945326\n",
      "Epoch: 151 Batch: 200, loss: 0.0009520550718350883\n",
      "Epoch: 151 Batch: 300, loss: 0.13598841322996674\n",
      "Epoch: 151 Batch: 400, loss: 0.007427498430291169\n",
      "Epoch: 151 Batch: 500, loss: 1.02196313603075\n",
      "Epoch: 152 Batch: 0, loss: 0.28747892724303903\n",
      "Epoch: 152 Batch: 100, loss: 0.3163649874063253\n",
      "Epoch: 152 Batch: 200, loss: 0.0013702943218120434\n",
      "Epoch: 152 Batch: 300, loss: 0.41023545355214347\n",
      "Epoch: 152 Batch: 400, loss: 0.230544341287305\n",
      "Epoch: 152 Batch: 500, loss: 0.038273689828419444\n",
      "Epoch: 153 Batch: 0, loss: 0.055429991284628284\n",
      "Epoch: 153 Batch: 100, loss: 0.00017963831323251943\n",
      "Epoch: 153 Batch: 200, loss: 0.015133753608091162\n",
      "Epoch: 153 Batch: 300, loss: 0.10503405921124541\n",
      "Epoch: 153 Batch: 400, loss: 0.11271364403039082\n",
      "Epoch: 153 Batch: 500, loss: 0.9243823335842164\n",
      "Epoch: 154 Batch: 0, loss: 0.002840370188553014\n",
      "Epoch: 154 Batch: 100, loss: 0.14697729374140164\n",
      "Epoch: 154 Batch: 200, loss: 0.04916282206443342\n",
      "Epoch: 154 Batch: 300, loss: 0.0004112099582077213\n",
      "Epoch: 154 Batch: 400, loss: 0.006588958977886705\n",
      "Epoch: 154 Batch: 500, loss: 0.09768859289330731\n",
      "Epoch: 155 Batch: 0, loss: 0.19601580203863359\n",
      "Epoch: 155 Batch: 100, loss: 0.19109295405497834\n",
      "Epoch: 155 Batch: 200, loss: 0.10262241830053234\n",
      "Epoch: 155 Batch: 300, loss: 0.0013817228096481072\n",
      "Epoch: 155 Batch: 400, loss: 0.40398493636119687\n",
      "Epoch: 155 Batch: 500, loss: 0.009781945272907263\n",
      "Epoch: 156 Batch: 0, loss: 0.6777838313007992\n",
      "Epoch: 156 Batch: 100, loss: 0.27947800106042553\n",
      "Epoch: 156 Batch: 200, loss: 0.00047654735677239214\n",
      "Epoch: 156 Batch: 300, loss: 0.8099719921241126\n",
      "Epoch: 156 Batch: 400, loss: 0.5297397838223368\n",
      "Epoch: 156 Batch: 500, loss: 0.346165667887345\n",
      "Epoch: 157 Batch: 0, loss: 0.10048057721271153\n",
      "Epoch: 157 Batch: 100, loss: 0.09566791430104649\n",
      "Epoch: 157 Batch: 200, loss: 0.5059838002665947\n",
      "Epoch: 157 Batch: 300, loss: 0.12663059693591547\n",
      "Epoch: 157 Batch: 400, loss: 0.17973858030351675\n",
      "Epoch: 157 Batch: 500, loss: 0.05609128917319861\n",
      "Epoch: 158 Batch: 0, loss: 0.22534852804223948\n",
      "Epoch: 158 Batch: 100, loss: 0.21445829990410867\n",
      "Epoch: 158 Batch: 200, loss: 0.28208157875997814\n",
      "Epoch: 158 Batch: 300, loss: 0.4059861725591088\n",
      "Epoch: 158 Batch: 400, loss: 0.022887538452536285\n",
      "Epoch: 158 Batch: 500, loss: 0.03606875697105138\n",
      "Epoch: 159 Batch: 0, loss: 0.003541990037335812\n",
      "Epoch: 159 Batch: 100, loss: 0.59129038932166\n",
      "Epoch: 159 Batch: 200, loss: 0.08755451438616978\n",
      "Epoch: 159 Batch: 300, loss: 0.24402925509993184\n",
      "Epoch: 159 Batch: 400, loss: 0.027147566770269582\n",
      "Epoch: 159 Batch: 500, loss: 0.04335941435622652\n",
      "Epoch: 160 Batch: 0, loss: 0.038440690616569115\n",
      "Epoch: 160 Batch: 100, loss: 0.047032454893542096\n",
      "Epoch: 160 Batch: 200, loss: 0.1862183912471431\n",
      "Epoch: 160 Batch: 300, loss: 0.24616704732035846\n",
      "Epoch: 160 Batch: 400, loss: 0.6878117644019239\n",
      "Epoch: 160 Batch: 500, loss: 0.2962124232980013\n",
      "Epoch: 161 Batch: 0, loss: 0.021448764264317067\n",
      "Epoch: 161 Batch: 100, loss: 0.5851204064596134\n",
      "Epoch: 161 Batch: 200, loss: 0.29187048255541825\n",
      "Epoch: 161 Batch: 300, loss: 1.133313577638172\n",
      "Epoch: 161 Batch: 400, loss: 0.37625862410014704\n",
      "Epoch: 161 Batch: 500, loss: 0.06829169197841907\n",
      "Epoch: 162 Batch: 0, loss: 0.05675737711205935\n",
      "Epoch: 162 Batch: 100, loss: 0.0010498482228681906\n",
      "Epoch: 162 Batch: 200, loss: 0.424271477585275\n",
      "Epoch: 162 Batch: 300, loss: 0.3852934316747058\n",
      "Epoch: 162 Batch: 400, loss: 0.04653981984330047\n",
      "Epoch: 162 Batch: 500, loss: 0.0014286637426107252\n",
      "Epoch: 163 Batch: 0, loss: 0.050384804516397264\n",
      "Epoch: 163 Batch: 100, loss: 0.0017925942371707214\n",
      "Epoch: 163 Batch: 200, loss: 0.27681906129680445\n",
      "Epoch: 163 Batch: 300, loss: 0.29758849851662045\n",
      "Epoch: 163 Batch: 400, loss: 0.0004682290684304983\n",
      "Epoch: 163 Batch: 500, loss: 0.23035671406450695\n",
      "Epoch: 164 Batch: 0, loss: 0.30685659831642065\n",
      "Epoch: 164 Batch: 100, loss: 0.00962111448879868\n",
      "Epoch: 164 Batch: 200, loss: 0.6313796580732759\n",
      "Epoch: 164 Batch: 300, loss: 0.004957611515124952\n",
      "Epoch: 164 Batch: 400, loss: 0.03991886692077335\n",
      "Epoch: 164 Batch: 500, loss: 0.03879301647625856\n",
      "Epoch: 165 Batch: 0, loss: 0.03874762639593434\n",
      "Epoch: 165 Batch: 100, loss: 0.3990129134259976\n",
      "Epoch: 165 Batch: 200, loss: 0.0002685280469382084\n",
      "Epoch: 165 Batch: 300, loss: 0.11322141204219253\n",
      "Epoch: 165 Batch: 400, loss: 0.00018073316329185067\n",
      "Epoch: 165 Batch: 500, loss: 0.03566431655420723\n",
      "Epoch: 166 Batch: 0, loss: 0.2894744339425724\n",
      "Epoch: 166 Batch: 100, loss: 0.09462201928101607\n",
      "Epoch: 166 Batch: 200, loss: 0.0028850450945292587\n",
      "Epoch: 166 Batch: 300, loss: 0.08281334996524824\n",
      "Epoch: 166 Batch: 400, loss: 0.02815454058013791\n",
      "Epoch: 166 Batch: 500, loss: 0.6785477242336343\n",
      "Epoch: 167 Batch: 0, loss: 0.3139239147773208\n",
      "Epoch: 167 Batch: 100, loss: 0.5909172923105522\n",
      "Epoch: 167 Batch: 200, loss: 0.011476240088241987\n",
      "Epoch: 167 Batch: 300, loss: 0.37663016225711066\n",
      "Epoch: 167 Batch: 400, loss: 0.06993201635178797\n",
      "Epoch: 167 Batch: 500, loss: 0.16922321674508511\n",
      "Epoch: 168 Batch: 0, loss: 0.6998227261970171\n",
      "Epoch: 168 Batch: 100, loss: 0.8086818422182638\n",
      "Epoch: 168 Batch: 200, loss: 0.0008766829917643159\n",
      "Epoch: 168 Batch: 300, loss: 0.09487012009893998\n",
      "Epoch: 168 Batch: 400, loss: 1.1853291049202\n",
      "Epoch: 168 Batch: 500, loss: 0.2799506056574967\n",
      "Epoch: 169 Batch: 0, loss: 0.22731358736829071\n",
      "Epoch: 169 Batch: 100, loss: 0.032032205993833794\n",
      "Epoch: 169 Batch: 200, loss: 0.3844791403415073\n",
      "Epoch: 169 Batch: 300, loss: 0.008342684327336095\n",
      "Epoch: 169 Batch: 400, loss: 0.2587086508485389\n",
      "Epoch: 169 Batch: 500, loss: 0.38484713642614504\n",
      "Epoch: 170 Batch: 0, loss: 0.3191309953816956\n",
      "Epoch: 170 Batch: 100, loss: 0.2564769421658295\n",
      "Epoch: 170 Batch: 200, loss: 0.1192869831915846\n",
      "Epoch: 170 Batch: 300, loss: 0.16301270897974468\n",
      "Epoch: 170 Batch: 400, loss: 0.10193323452263581\n",
      "Epoch: 170 Batch: 500, loss: 0.20330776598118477\n",
      "Epoch: 171 Batch: 0, loss: 0.3607242524656057\n",
      "Epoch: 171 Batch: 100, loss: 0.39263380333914466\n",
      "Epoch: 171 Batch: 200, loss: 0.3905970401715403\n",
      "Epoch: 171 Batch: 300, loss: 0.0002921085347270132\n",
      "Epoch: 171 Batch: 400, loss: 0.0002467984590693341\n",
      "Epoch: 171 Batch: 500, loss: 0.5928675875677658\n",
      "Epoch: 172 Batch: 0, loss: 0.3948628400374126\n",
      "Epoch: 172 Batch: 100, loss: 0.8626719093737404\n",
      "Epoch: 172 Batch: 200, loss: 0.8967593196528812\n",
      "Epoch: 172 Batch: 300, loss: 2.0709356246198886e-05\n",
      "Epoch: 172 Batch: 400, loss: 0.22326482798301509\n",
      "Epoch: 172 Batch: 500, loss: 0.00965326929334916\n",
      "Epoch: 173 Batch: 0, loss: 0.5346162353927001\n",
      "Epoch: 173 Batch: 100, loss: 0.6210574258884796\n",
      "Epoch: 173 Batch: 200, loss: 0.2062740756690594\n",
      "Epoch: 173 Batch: 300, loss: 0.3063370640480888\n",
      "Epoch: 173 Batch: 400, loss: 0.6813474649493905\n",
      "Epoch: 173 Batch: 500, loss: 0.0019333853406345125\n",
      "Epoch: 174 Batch: 0, loss: 0.03511703700828464\n",
      "Epoch: 174 Batch: 100, loss: 0.3363496458340522\n",
      "Epoch: 174 Batch: 200, loss: 0.00650325223467783\n",
      "Epoch: 174 Batch: 300, loss: 3.7777343425763514\n",
      "Epoch: 174 Batch: 400, loss: 0.0896032029383041\n",
      "Epoch: 174 Batch: 500, loss: 1.2189136404836454\n",
      "Epoch: 175 Batch: 0, loss: 0.10206251492090547\n",
      "Epoch: 175 Batch: 100, loss: 0.12050354033199463\n",
      "Epoch: 175 Batch: 200, loss: 0.0009750901641098283\n",
      "Epoch: 175 Batch: 300, loss: 0.03623710378103051\n",
      "Epoch: 175 Batch: 400, loss: 0.461020145066342\n",
      "Epoch: 175 Batch: 500, loss: 0.12977249685056733\n",
      "Epoch: 176 Batch: 0, loss: 0.0746336020721786\n",
      "Epoch: 176 Batch: 100, loss: 0.047121275067427464\n",
      "Epoch: 176 Batch: 200, loss: 7.880863442925242\n",
      "Epoch: 176 Batch: 300, loss: 0.5047976457708904\n",
      "Epoch: 176 Batch: 400, loss: 0.8807245996280417\n",
      "Epoch: 176 Batch: 500, loss: 0.15454655153570337\n",
      "Epoch: 177 Batch: 0, loss: 0.17384601476127426\n",
      "Epoch: 177 Batch: 100, loss: 0.19384913427108194\n",
      "Epoch: 177 Batch: 200, loss: 0.10094601955732957\n",
      "Epoch: 177 Batch: 300, loss: 0.039372999900179874\n",
      "Epoch: 177 Batch: 400, loss: 0.0011810740233267221\n",
      "Epoch: 177 Batch: 500, loss: 0.003648604699240123\n",
      "Epoch: 178 Batch: 0, loss: 0.027972551564535573\n",
      "Epoch: 178 Batch: 100, loss: 0.0012523816810273653\n",
      "Epoch: 178 Batch: 200, loss: 0.0078729421816685\n",
      "Epoch: 178 Batch: 300, loss: 0.0009233808290951345\n",
      "Epoch: 178 Batch: 400, loss: 1.1064813273351377\n",
      "Epoch: 178 Batch: 500, loss: 0.6196373899462526\n",
      "Epoch: 179 Batch: 0, loss: 0.0015408539237381077\n",
      "Epoch: 179 Batch: 100, loss: 0.5242750557808533\n",
      "Epoch: 179 Batch: 200, loss: 0.09750931120086588\n",
      "Epoch: 179 Batch: 300, loss: 7.892453868252451\n",
      "Epoch: 179 Batch: 400, loss: 0.058988335474175826\n",
      "Epoch: 179 Batch: 500, loss: 0.32038004758765687\n",
      "Epoch: 180 Batch: 0, loss: 0.9215860967986931\n",
      "Epoch: 180 Batch: 100, loss: 0.0003064140579806885\n",
      "Epoch: 180 Batch: 200, loss: 0.8079328019634215\n",
      "Epoch: 180 Batch: 300, loss: 0.35480833584302635\n",
      "Epoch: 180 Batch: 400, loss: 0.31162635458332055\n",
      "Epoch: 180 Batch: 500, loss: 0.0017935700020979618\n",
      "Epoch: 181 Batch: 0, loss: 0.02284728089060162\n",
      "Epoch: 181 Batch: 100, loss: 0.17253099119186482\n",
      "Epoch: 181 Batch: 200, loss: 0.29920806148429446\n",
      "Epoch: 181 Batch: 300, loss: 0.00030161942751394546\n",
      "Epoch: 181 Batch: 400, loss: 0.0058771913022449915\n",
      "Epoch: 181 Batch: 500, loss: 0.188463901292477\n",
      "Epoch: 182 Batch: 0, loss: 0.5310526095414956\n",
      "Epoch: 182 Batch: 100, loss: 0.5920634159415277\n",
      "Epoch: 182 Batch: 200, loss: 0.05360323196081576\n",
      "Epoch: 182 Batch: 300, loss: 0.232357653050388\n",
      "Epoch: 182 Batch: 400, loss: 0.34088339307089577\n",
      "Epoch: 182 Batch: 500, loss: 0.02974053202976352\n",
      "Epoch: 183 Batch: 0, loss: 0.20420711733333807\n",
      "Epoch: 183 Batch: 100, loss: 0.05349739613199185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183 Batch: 200, loss: 0.2787535311794321\n",
      "Epoch: 183 Batch: 300, loss: 0.3449031089255375\n",
      "Epoch: 183 Batch: 400, loss: 0.013279747544024416\n",
      "Epoch: 183 Batch: 500, loss: 0.012164131335510975\n",
      "Epoch: 184 Batch: 0, loss: 0.3654014557531331\n",
      "Epoch: 184 Batch: 100, loss: 0.9575360609194581\n",
      "Epoch: 184 Batch: 200, loss: 0.28221577276570803\n",
      "Epoch: 184 Batch: 300, loss: 0.48336030829854826\n",
      "Epoch: 184 Batch: 400, loss: 1.02577815545914\n",
      "Epoch: 184 Batch: 500, loss: 0.002090633552164964\n",
      "Epoch: 185 Batch: 0, loss: 0.10045644586035474\n",
      "Epoch: 185 Batch: 100, loss: 0.03464699575405779\n",
      "Epoch: 185 Batch: 200, loss: 0.3768985187933983\n",
      "Epoch: 185 Batch: 300, loss: 0.19819080990543378\n",
      "Epoch: 185 Batch: 400, loss: 0.3585186604879009\n",
      "Epoch: 185 Batch: 500, loss: 0.0036082252606026385\n",
      "Epoch: 186 Batch: 0, loss: 0.23427933359522674\n",
      "Epoch: 186 Batch: 100, loss: 0.0045014392507217536\n",
      "Epoch: 186 Batch: 200, loss: 0.15999892598378662\n",
      "Epoch: 186 Batch: 300, loss: 0.05778087387542901\n",
      "Epoch: 186 Batch: 400, loss: 0.37599283704225694\n",
      "Epoch: 186 Batch: 500, loss: 0.03145017756821988\n",
      "Epoch: 187 Batch: 0, loss: 0.0025870693967021824\n",
      "Epoch: 187 Batch: 100, loss: 0.10786311041647573\n",
      "Epoch: 187 Batch: 200, loss: 0.004182859001869067\n",
      "Epoch: 187 Batch: 300, loss: 0.40436875406021744\n",
      "Epoch: 187 Batch: 400, loss: 0.15605874962797509\n",
      "Epoch: 187 Batch: 500, loss: 0.007287806617751198\n",
      "Epoch: 188 Batch: 0, loss: 0.36482579515933755\n",
      "Epoch: 188 Batch: 100, loss: 0.29434628248152067\n",
      "Epoch: 188 Batch: 200, loss: 0.05317108998798875\n",
      "Epoch: 188 Batch: 300, loss: 0.07792811211755382\n",
      "Epoch: 188 Batch: 400, loss: 0.506017848334019\n",
      "Epoch: 188 Batch: 500, loss: 0.28545333387704286\n",
      "Epoch: 189 Batch: 0, loss: 0.5033400201765724\n",
      "Epoch: 189 Batch: 100, loss: 1.4227544620003676\n",
      "Epoch: 189 Batch: 200, loss: 0.1917344628487794\n",
      "Epoch: 189 Batch: 300, loss: 0.1487404770342389\n",
      "Epoch: 189 Batch: 400, loss: 0.026728173334942065\n",
      "Epoch: 189 Batch: 500, loss: 0.03628569727218202\n",
      "Epoch: 190 Batch: 0, loss: 0.07304557022226556\n",
      "Epoch: 190 Batch: 100, loss: 0.03824466215808958\n",
      "Epoch: 190 Batch: 200, loss: 9.075825923704944e-06\n",
      "Epoch: 190 Batch: 300, loss: 1.1158607900006416\n",
      "Epoch: 190 Batch: 400, loss: 0.2020130749257625\n",
      "Epoch: 190 Batch: 500, loss: 0.2008408376304065\n",
      "Epoch: 191 Batch: 0, loss: 0.04094363532528527\n",
      "Epoch: 191 Batch: 100, loss: 0.07750547157394141\n",
      "Epoch: 191 Batch: 200, loss: 0.01324016267469698\n",
      "Epoch: 191 Batch: 300, loss: 0.1263552094774805\n",
      "Epoch: 191 Batch: 400, loss: 0.0038052369648614677\n",
      "Epoch: 191 Batch: 500, loss: 2.1591338515346705\n",
      "Epoch: 192 Batch: 0, loss: 0.019703526929621455\n",
      "Epoch: 192 Batch: 100, loss: 0.019955497207186063\n",
      "Epoch: 192 Batch: 200, loss: 2.4723505307704516\n",
      "Epoch: 192 Batch: 300, loss: 0.10391856855878481\n",
      "Epoch: 192 Batch: 400, loss: 2.481827496024957\n",
      "Epoch: 192 Batch: 500, loss: 0.1998230769575604\n",
      "Epoch: 193 Batch: 0, loss: 0.26302370681912385\n",
      "Epoch: 193 Batch: 100, loss: 0.3333867214311483\n",
      "Epoch: 193 Batch: 200, loss: 0.008050993349419618\n",
      "Epoch: 193 Batch: 300, loss: 1.90631857519008\n",
      "Epoch: 193 Batch: 400, loss: 0.9635400365278544\n",
      "Epoch: 193 Batch: 500, loss: 0.5927087635751457\n",
      "Epoch: 194 Batch: 0, loss: 0.2726710622917818\n",
      "Epoch: 194 Batch: 100, loss: 0.28732293569590145\n",
      "Epoch: 194 Batch: 200, loss: 0.027373749101406927\n",
      "Epoch: 194 Batch: 300, loss: 0.13272931622575224\n",
      "Epoch: 194 Batch: 400, loss: 0.2636226523975096\n",
      "Epoch: 194 Batch: 500, loss: 0.7980805486256958\n",
      "Epoch: 195 Batch: 0, loss: 0.01780489030632303\n",
      "Epoch: 195 Batch: 100, loss: 0.09272788434128824\n",
      "Epoch: 195 Batch: 200, loss: 0.5320025125461089\n",
      "Epoch: 195 Batch: 300, loss: 0.5721944456510419\n",
      "Epoch: 195 Batch: 400, loss: 0.8916420132880299\n",
      "Epoch: 195 Batch: 500, loss: 0.0036852638911512078\n",
      "Epoch: 196 Batch: 0, loss: 0.05328288042308298\n",
      "Epoch: 196 Batch: 100, loss: 0.14761882276659774\n",
      "Epoch: 196 Batch: 200, loss: 0.28903569384177236\n",
      "Epoch: 196 Batch: 300, loss: 0.029478540104940373\n",
      "Epoch: 196 Batch: 400, loss: 0.07183737445506344\n",
      "Epoch: 196 Batch: 500, loss: 0.3828720074092909\n",
      "Epoch: 197 Batch: 0, loss: 0.041938246545039685\n",
      "Epoch: 197 Batch: 100, loss: 0.20326983156063885\n",
      "Epoch: 197 Batch: 200, loss: 0.48872610111584636\n",
      "Epoch: 197 Batch: 300, loss: 0.14439607778636643\n",
      "Epoch: 197 Batch: 400, loss: 0.07250284870708787\n",
      "Epoch: 197 Batch: 500, loss: 0.024869158253750922\n",
      "Epoch: 198 Batch: 0, loss: 0.04149860828725957\n",
      "Epoch: 198 Batch: 100, loss: 0.2695244072652399\n",
      "Epoch: 198 Batch: 200, loss: 0.07598870660313459\n",
      "Epoch: 198 Batch: 300, loss: 0.10965806601041479\n",
      "Epoch: 198 Batch: 400, loss: 0.14265593475811889\n",
      "Epoch: 198 Batch: 500, loss: 0.09272905806786268\n",
      "Epoch: 199 Batch: 0, loss: 0.002233303826280014\n",
      "Epoch: 199 Batch: 100, loss: 0.6557943314991379\n",
      "Epoch: 199 Batch: 200, loss: 0.18683263249207002\n",
      "Epoch: 199 Batch: 300, loss: 0.34043931304967384\n",
      "Epoch: 199 Batch: 400, loss: 0.01072236446365893\n",
      "Epoch: 199 Batch: 500, loss: 0.20944213438236603\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Linear Regression: 实现了回归，其中包括线性函数的定义，为什么要用线性函数，loss的意义，梯度下降的意义，stochastic gradient descent\n",
    "Use Boston house price dataset.\n",
    "北京2020年房价的数据集，为什么我没有用北京房价的数据集呢？\n",
    "Boston: room size, subway, highway, crime rate 有一个比较明显的关系，所以就观察关系比较容易\n",
    "北京的房价：！远近，！房况 ==》 学区！！！！ => 非常贵 海淀区\n",
    "Harder than deep learning:\n",
    "    1. compiler\n",
    "    2. programming language & automata\n",
    "    3. computer graphic\n",
    "    4. complexity system\n",
    "    5. computing complexity\n",
    "    6. operating system\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linear_regression_course import train\n",
    "import pickle\n",
    "\n",
    "\n",
    "dataset = load_boston()\n",
    "data = dataset['data']\n",
    "target = dataset['target']\n",
    "columns = dataset['feature_names']\n",
    "\n",
    "dataframe = pd.DataFrame(data)\n",
    "dataframe.columns = columns\n",
    "dataframe['price'] = target\n",
    "\n",
    "# print(dataframe.corr()) # show the correlation of dataframe variables\n",
    "# correlation => 如果一个值的增大，会引起另外一个值一定增大，而且是定比例增大 相关系数就越接近于1\n",
    "# correlation => 0 就是两者之间没有任何关系\n",
    "# correlation => -1 一个值增大 另外一个值一定减小 而且减小是成相等比例的\n",
    "\n",
    "# sns.heatmap(dataframe.corr())\n",
    "# plt.show()\n",
    "\n",
    "# RM：小区平均的卧室个数\n",
    "# LSTAT: 低收入人群在周围的比例\n",
    "\n",
    "rm = dataframe['RM']\n",
    "lstat = dataframe['LSTAT']\n",
    "price = dataframe['price']\n",
    "greater_then_most = np.percentile(price, 66)\n",
    "dataframe['expensive'] = dataframe['price'].apply(lambda p: int(p > greater_then_most))\n",
    "target = dataframe['expensive']\n",
    "\n",
    "print(dataframe[:20])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def model(x, w, b):\n",
    "    return sigmoid(np.dot(x, w.T) + b)\n",
    "\n",
    "\n",
    "def loss(yhat, y):\n",
    "    return -np.sum(y*np.log(yhat) + (1 - y)*np.log(1 - yhat))\n",
    "\n",
    "\n",
    "def partial_w(x, y, yhat):\n",
    "    return np.array([np.sum((yhat - y) * x[0]), np.sum((yhat - y) * x[1])])\n",
    "\n",
    "\n",
    "def partial_b(x, y, yhat):\n",
    "    return np.sum((yhat - y))\n",
    "\n",
    "\n",
    "model, w, b, losses = train(model, target, loss, partial_w, partial_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb3ce75668>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD6CAYAAABXh3cLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHVxJREFUeJzt3XmQm/d93/H3F9cCCyyw98ljJVIURer2ypV1RJZUSXYs27XdpK7rxIknUeup48y448xkpkfaPzrjdqYTZ5xxR5506tjpeOpEimXZrm3Jp2Ix0lLUQZESKZ7iXsTeF7C4fv1jwdVyuSK52AN4gM9rRkMsgF189OCHDx788BzmnENERLzJV+4AIiJSOpW4iIiHqcRFRDxMJS4i4mEqcRERD1OJi4h4mEpcRMTDVOIiIh6mEhcR8bDAZj9Aa2ur6+3t3eyHERGpKgcPHhx1zrVd6X6bXuK9vb309/dv9sOIiFQVMztzNffTdIqIiIepxEVEPEwlLiLiYSpxEREPU4mLiHiYSlxExMNU4iIiHlbxJf78iTEOD0yVO4aISEWq6BKfz+R47Jv9/NcfHC13FBGRilTRJf7Uy4PMpHOcGZsvdxQRkYpUsSXunOOvn1/c63RwKkU6my9zIhGRylOxJX7o7UmODE1zR28TzsG5Ca2Ni4isVLElXig47tndyh8/uAdAUyoiIqu4qhI3s6CZfW/Zz98wswNm9pSZbcqREPt6m/nWH/wT9nXHATitEhcRucQVS9zMIsBB4KHiz/cAAefcnUAceHgzAzbVB2kIBzgzNreZDyMi4klXLHHnXMo5dzNwrnjVCPCVq/399TIzdrbUazpFRGQVa54Kcc4dBzCzjwEF4Mcr72NmjwGPAezYsWOdEWFnS5TXtcOPiMglSlqTNrOPAF8APuycy6283Tn3uHOuzznX19Z2xbMLXVFvSz3nJlLk8oV1/y0RkWqy5hI3s07gS8CjzrmZjY90qZ3NUXIFx+BkeiseTkTEM0pZE/8M0AX8yMyeM7PPbnCmS2xrigBwblLz4iIiy131nLhzbnfx3y8DX960RKtojoUAmJzPbuXDiohUvIrd2We55vrFEh+fy5Q5iYhIZfFEiTeqxEVEVuWJEg8FfDTUBVTiIiIreKLEAZqiISbmVeIiIst5psSboyGtiYuIrOCpEteauIjIxTxT4k31ISbmtImhiMhyninx5mhQ0ykiIit4psSboiFS2TypjE7TJiJygWdKfGmHH82Li4gs8UyJN0UXS3xCUyoiIks8U+ItUe21KSKykmdKfGlNXNMpIiJLPFPiOgiWiMilPFPi8UgQn2lOXERkOc+UuN9nNNaHtHWKiMgynilxgKb6oPbaFBFZxlMl3hwNMTq7UO4YIiIVw1Ml3pmIMDKtkyWLiFzgqRLvbgwzOJWmUHDljiIiUhE8VeI9jREyuQJj2kJFRATwWIl3JyIADE6mypxERKQyeKvEG1XiIiLLearEe4olPqASFxEBrrLEzSxoZt8rXg6b2dNm9oqZfdPMbHMjviMeCVAf8jM4qS1URETgKkrczCLAQeCh4lWfBs45524BmpZdv+nMjO7GiKZTRESKrljizrmUc+5m4FzxqgeAnxQv/xS4f5Oyraq7McLglEpcRARKmxNvAaaKl6eB5pV3MLPHzKzfzPqTyeR68l2ipzGsNXERkaJSSnwUSBQvJ4o/X8Q597hzrs8519fW1raefJfoTkQYnc2QzupcmyIipZT4s8DDxcsPAD/buDhXdmEzw6EpfbkpIlJKif8N0GNmrwLjLJb6llkqcU2piIgQuNo7Oud2F/9dAB7dtERXoG3FRUTe4amdfQA6EnWYoW3FRUTwYInXBfy0xeq0hYqICB4scYAubSsuIgJ4tMR7GsOaExcRwaMl3p1Y3PXeOZ0cQkRqmzdLvDFCOltgYl4nTRaR2ubZEgcdV1xExJMlrm3FRUQWebLEuxvDgNbERUQ8WeLN0RB1AZ9KXERqnidL3MzoaYwwqINgiUiN82SJw+KXm+cmtCYuIrXNsyW+qy3KWyMzFAraVlxEapdnS3x/d4K5TJ7TY3PljiIiUjbeLfGeOACvD06XOYmISPl4tsSva28g6DcOD05d+c4iIlXKsyUeCvjY09HAEa2Ji0gN82yJA9zYneDwwJQOhCUiNcvTJb6/J87EfFYnTRaRmuXtEu9OAHB4QPPiIlKbPF3i13c2AHBsZKbMSUREysPTJR6rC7CtKcKbI7PljiIiUhaeLnGA6zsaODasNXERqU2eL/E9nQ2cSM6SyRXKHUVEZMt5vsT3djaQKzjtfi8iNamkEjezqJl918z+wcz+20aHWos9HYtfbr6pKRURqUGlron/K+CAc+5uYL+Z3bCBmdbk2rYofp9pCxURqUmllvgkEDMzPxABMhsXaW3qAn6uaY1qTVxEalKpJf4k8AHgBHDUOXdi+Y1m9piZ9ZtZfzKZXG/GK9rTEeP4eW1mKCK1p9QS/1Pga865XqDZzO5afqNz7nHnXJ9zrq+trW29Ga9oe1M9A5MpHUNFRGpOqSXeAFw4YMkCENuYOKXpSoTJ5AqMzZVtVkdEpCxKLfG/BD5nZs+zOCf+7MZFWruuxggAQ5M6EJaI1JZAKb/knDsN3L2xUUrXlQgDMDSV4qZtiTKnERHZOp7f2QegK1FcE9chaUWkxlRFibdEQ4T8PganUuWOIiKypaqixH0+oyNRx7DWxEWkxlRFicPilIq+2BSRWlM1Jd6dCGs6RURqTtWUeFdjhJHpNIWCdvgRkdpRPSWeCJPNO0bnFsodRURky1RRiWuHHxGpPVVU4u/s8CMiUiuqpsS3N9cDcEwnTRaRGlI1JZ6IBNnfHee5t0bLHUVEZMtUTYkD3HNdK4fOTjC3kCt3FBGRLVFVJX7v7jayeccLp8bLHUVEZEtUVYn39TZRF/Dxq+OaUhGR2lBVJR4O+nnvNc0899bmnxJORKQSVFWJA9y8LcGJ5Bx57bkpIjWg6kq8Mx4mX3CMzWrPTRGpflVX4h3xxZ1+RqZV4iJS/aq2xIentfu9iFS/qivxzoRKXERqR9WVeGusDr/PGNFZfkSkBlRdift9RlusTmviIlITqq7EATridYyoxEWkBlRpiYdV4iJSE0oucTP7EzM7YGY/NLPQRoZar85EWGe+F5GaUFKJm9m1wH7n3J3AD4FtG5pqnTriYabTOVKZfLmjiIhsqlLXxB8Emszsl8C9wKmNi7R+ndpWXERqRKkl3gYknXO/weJa+D3LbzSzx8ys38z6k8mtPxjV0g4/mlIRkSpXaolPA28WL58Eepbf6Jx73DnX55zra2trW0++knQm6gD05aaIVL1SS/wg0Fe8vJvFIq8YncUz3w9M6qTJIlLdSipx59zzwJiZvQi86Zx7YWNjrU+sLkB3IsybwzPljiIisqkCpf6ic+5zGxlko+3rjnNkaLrcMURENlVV7uwDsK87wcnkrDYzFJGqVr0l3hWn4OCNYa2Ni0j1qtoS398dB9CUiohUtaot8W1NERrCAY4MqsRFpHpVbYmbGfu64ryuEheRKla1JQ6wvzvB0aFpJucz5Y4iIrIpqrrEf/uObWTzBf7i2bfKHUVEZFNUdYnv7YzzL+7Yzl8/f5qTydlyxxER2XBVXeIAX3zoeszgOwfPlTuKiMiGq/oSb2uoo6cxwtnx+XJHERHZcFVf4gA9TREGJnQwLBGpPrVR4o0RHdFQRKpSjZR4PcmZBdJZHUdFRKpLbZR40+LxxYd0ph8RqTK1UeKNxZNEaF5cRKpMTZT4tqYLZ/rRFioiUl1qosQ7E2F8pjVxEak+NVHiQb+PjniYc9pCRUSqTE2UOBQ3M9SauIhUmdop8SZtKy4i1ad2SrwxwvBUmnzBlTuKiMiGqZkS39ZUT67gGJ7WtuIiUj1qpsR3NNcDcHZMmxmKSPWouRJ/W0czFJEqUjMl3tUYxu8zHZJWRKrKukrczL5oZs9sVJjNFPT76G4Mq8RFpKqUXOJmthP4zAZm2XQ7mutV4iJSVdazJv4V4E83KshW2NFcrzlxEakqJZW4mX0KeAU48i63P2Zm/WbWn0wm15NvQ21vrmdsLsPsQq7cUURENkSpa+KPAg8C3wbeY2afX36jc+5x51yfc66vra1tvRk3jLZQEZFqU1KJO+c+5Zy7B/gkcNA599WNjbU5VOIiUm1qZhNDWLbDj0pcRKpEYD2/7Jw7DfzTjYmy+RKRIA3hAGe016aIVImaWhM3M65ti3FqdK7cUURENkRNlTjArtYoJ5Kz5Y4hIrIhaq/E22MMTaW1maGIVIXaK/G2KACnkppSERHvq8ESjwFoSkVEqkLNlfiOlnr8PuOkSlxEqkDNlXhdwM/2pggnNJ0iIlWg5kocFqdUNJ0iItWgNku8PcbJ0TmdNFlEPK82S7wtSiZX0DFURMTzarLE93bGAXhjeKbMSURE1qcmS3xPRwNm8MbwdLmjiIisS02WeCTk55qWKG8MaU1cRLytJkscYG9Xg9bERcTzarfEO+OcGZ9nTsdQEREPq+ESb8A5ODaiKRUR8a6aLfEburSFioh4X82WeE9jhFhdgKNDmhcXEe+q2RL3+YxdbVGd5UdEPK1mSxxgR0tU59sUEU+r6RLvbalnYDJFNl8odxQRkZLUdInvaK4nX3AMTKTKHUVEpCQ1XeK9rYunajs9pnlxEfGmmi7xnc31AJzV0QxFxKNKLnEz+4aZHTCzp8wssJGhtkpbQx2RoF9fboqIZ5VU4mZ2DxBwzt0JxIGHNzTVFjEzdrbUc0bTKSLiUaWuiY8AX1nn36gIO5rrtSYuIp5VUgE75447514ws48BBeDHy283s8fMrN/M+pPJ5Ebk3DS9rVHOjM9T0KnaRMSD1jMn/hHgC8CHnXMXHQrQOfe4c67POdfX1ta23oybakdzPZlcgeHpdLmjiIisWalz4p3Al4BHnXOePoLUrrYYAMfPz5Y5iYjI2pW6Jv4ZoAv4kZk9Z2af3cBMW2pf8WiGOhCWiHhRSZsGOue+DHx5g7OURaI+SHcirBIXEU/y9JYlG+WGrrhKXEQ8SSXOYomfSM6RzubLHUVEZE1U4iyWeL7geEtfboqIx6jEgRu6GgA4oikVEfEYlTiwsyVKJOjXvLiIeI5KHPD7jJt6ErxwarzcUURE1kQlXvTw/g5eH5zWwbBExFNU4kUfvKkLgO+/NlTmJCIiV08lXtTTGOGW7Y38QCUuIh6iEl/mQzd1cnhAUyoi4h0q8WU+dHM3ZvDkoYFyRxERuSoq8WV6GiO879oWnnhpAOd0fHERqXwq8RU+fvs2zo7P039motxRRESuSCW+wgdv7KQ+5Odv+8+VO4qIyBWpxFeI1gX4yC3dPPnyAOdndLYfEalsKvFV/Jv7dpHLF/j6L0+WO4qIyGWpxFfR2xrlo7f28K0DZzmvc2+KSAVTib+Lzz+wG4fjX379ACMqchGpUCrxd7GrLcY3fv+9DE+lue+//4zP/u8X+cWxpDY9FJGKYptdSn19fa6/v39TH2MzvTE8zbdfeJsfvT7M0FSam3oSPLK/A7/PRywc4J/fvo1IyF/umLJFcvkCAb/WfWTzmdlB51zfFe+nEr86C7k8//fFt/nOwXO8em5q6frWWB37uuPUBXzUBXy0xurobgzT3RihuzFCPBxkcDLFZCqLc449HQ3EI0HGZhd46/wsu9pi3LK98bKPncsXMDP8PmMmneU/f+8I+7vj/O77evH7jNmFHKMzC+xsqcfMln7POcf4XIbmaOii61canV0gEQkSXFFOhYLjyNA0Q1Np7r2ulXDwnTer06NzPHFogHyhwC3bGnloXwczCzmCPt+a39SGp9JEgn4S9cGLrp+azxLwG9G6xfN5nxqd4yvPHOPBGzr4wI2dl+QtxdBUiicPDdCdiPCRW7rx+d59OR08M87v/NUL/NZ7tvHvH9130eOnMnlCAR/+y/z+ZioUHN9+8W1+eHiIP37wOvp6my+5j3Nu1XHgnCOVzTObzjG7kGM+kyedzbOvO059KEC+4PAZlx1Dl/P64BTfOnCWu3e38Js3dmEGPz4ywunROX7v7l7qAovj5fx0mlg4QH3o4vO3X+ioC4+fnFng8OAUd+1qoS7gJ19wJS/3dDZPNl+gIRx819vrAr6S/9/XQyW+iSbnM4QCPl4fnObrvzzJ+ZkFFnIFFrJ5zs8sMLuQu+q/5TP4oweu40RylqND0/h9RiISZHtzPXs7G/j+a8O88vYkAHfvbmFyPsvrg4snr+hpjOCcY2g6jXPQnQjzvl2t7OuOEw35+fuXBzhwcpzr2mN0JsK8dX6W6zoa8Bm88vYkd17bgt9nPP3qEA11Ae65rpX79rRRXxfg9YEpnjw0wPmZBQCa6oN84MZObuiKc2RwmideGiBbKGBAwcH25giDk2kawgE+f/9u8gXH4cFpTpyfZW9nA+3xMEeHponVBehpitAZDzM0leLAyXFeG1h8U7ymNcrHb+thd3uMl85O8I1fnyHoNz5++zbu3t3Cf/neEQanFr+f6EqE+fSdO7lvTxvpbJ5fnxgj4De6EmFu6IozPpthZCZNNu84MjjN4GSKa1qjtMRCREIBQn7jmaPnefboCIXiS2B3e4yuRJjbdjTxB/deQ//pcQ6cHGdgIsV9e9r482eOMbuQYzqdY29nA5+4fRtm8I+nxvnZG+cxg+vaG3hkfyfT6SynRufY3hRhdC7D6dE5oqEAbQ3vvMkfHpjmV8eTdCbCNEdD5AsO52B8LsP5mQU+8Z4e9nXF+drPT3B2fJ6GcIA/vPdadrXHmE3nmEpleeHUOIcHp0hl8gwV3wzTuTz/7NYePnnHdkIBH6dG53jx9Dg/OTJCPBzksd+4ll+9NcrLZyeZXVgs7nzh0h5ojYW4o7eZn715no54mAf2thMPBwkFfERDfrobI8ykc5wcnWV0JsPYXIZUNset2xvZ352g4Bw/fn2Ep18dxMzIFxwd8TqidQFOJhePT7S3s4G7d7dydGiaX58YIxz0cee1LfS2RInW+UnOLPDM0fNMzmeI1QWI1QUYnk5TcHBta5SdLfX8/FiSD+zv5L3XNPPCqXH2dcXZ0VLPobOTHHp7ktGZBboSYXqaIvQ0RuhpinB9RwPnJlL8x+8eZjqdozUW4vfu6uX02Dz/7/Aw79vVwtxCjl+fGCPoN27b0cS//o1rOXR2kjeGZwDobgzT2xLlmtYojfVBzk2k+P6rQ5xIzrKQK3BjT5yP3trDI/s7114yqMTLajqdZXAyxeBkiqlUlu5EhJZYiHxhcXomnc2TiITY2VLP//jJseKLK8Bdu1oBmExlOD4yy9hcht6Weh69uZtsocDfHxpgcj7L1z59O7MLeZ5+ZZBYXYDe1ihN0RC/Opbk4JkJxuYywOKL8Lf6ttN/epyZdI7rOho4NjxDrlDgxp4EvziWJJ3N8zt37mQmnePnbyYZLn6J6/cZ91/fzm/e1ElzNMR3+s/xy2NJZhZyREN+PnhTF3/yyPU0R0M8cWiAp14e5KZtCQ6dneDAycWTa/Q0Rri2Lcrhgamlx09n8wxMpsjkCoQCPvZ3x3lkfyfOwa+OJ/n1iTEAzOBjt/WAg6dfGyKTKxAPB/g/f3gnw1Np/tc/nFq675XUh/x0JcK8PZ4iky8sXd8cDfHbfdv51Ht38NLZCb794lnmFvK8NjCF37dYOqGAj8ZIkPMzCwT9xt997i7OTaT46k/fWjqdX3tDHR++pZug38eLp8c5eGaCUMDHNS1Rzk3M0xQNsbs9RiqTJzmzwMBkioVcgVhdgPv3tjM5n2EqlcVX/LTVEA4Q9Pv4yZER4J2iOzI4zfMnL/5/bo2F6NvZTMBvvP/6dj5wYyd/8exxvnXgDPOZd078HQ35ef/17bw5MsNb52dpCAd4YG87jZEgsXCAWN2Ff/1EQwEKDr514AxHh6Z5aF8HA5MpXjg1zkKuwEoBn9EUDdESDRHwG0eHZpbeFBKRIJ+4fRt/9MBufnEsyS+OJRmby/DI/g7aG8L82VOvMzmfoSMe5qO39jA+t7D4xjmZIp3NEynm3tlcz+xCjpl0jp6mCLvaovz5M8eZSWd5//Xt/OC1IeYzeTrj4aUxHA76uHlbIz2NEYamUgxMphiaTJNb9oZ1+45GHt7fyYGTY/z8zSQhv4+H9nfQf3qcUMDHozd3Uyg4njg0QHJmAZ/Bno7F0zkOTKSYWbHC1hkPc8v2BH6f8drAFJ+8Ywf/9v7dVzVOV1KJe0Sh4Dhwaoxbtzde9DHSOcfwdJr2hvDSR8VsvsBMOkdzNPSuf885x8R8lrmFHG0NdRdNgay0kMuTL7ilx3XOcXJ0DuccbQ1hEpGLP2Lm8gXOzyzQGQ+/67SDc443hmfoiIeXchYKjlyxEC/8PDGfIREJXjK/PDCZYjqVpSUaoj0eBmA+k6P/9ATXtEbZ3ly/dN+3x+d55dwkhnHvnlZCfh9nxuZ5Y3ia1lgdXYnFZdfdGCHo95EvLE4bzGdypDJ5OhPhpY/yyx08M8F3Xx7grl0tPLC3g4DP+OXxxRf4Xbtbl+43OJkiEvTTWB+86OP26OwCsbrAuy575xyjsxkawu9+H4CXzk4wMpXm4f2dS2Pg6NA0qWyehroAsXCAjobVn4upVJYDJ8cIBXx0JyLsbo/h9xnZfIGXzkxwY09iaZpqLQoFR6Y4DgcmU0RDfq5pjV70PE6nswxMLL5R39AVX3reN8OFKaKJuQzT6Sw7W6KMTKcZm82wpyN2yfjKFxZfV0cGp5nP5PjQTV1L9zk2MkNDOEBXInLJ48xncvzizSS37WiiMxFeeuyxuQxnxuaYTuWI1gV4z86mi6Z2CgV32Sm6y9m0EjezMPC3wHbgVeB33WX+iEpcRGTtrrbES3mL/DRwzjl3C9AEPFTC3xARkQ1QSok/APykePmnwP0bF0dERNailBJvAS5sYzcNXLotk4iIbIlSSnwUSBQvJ4o/X8TMHjOzfjPrTyaT68knIiKXUUqJPws8XLz8APCzlXdwzj3unOtzzvW1tbWtJ5+IiFxGKSX+N0CPmb0KjLNY6iIiUgZr3lDUObcAPLoJWUREZI10JB8REQ/b9D02zSwJnCnx11tZ5YvTClGp2ZRrbSo1F1RuNuVam1Jz7XTOXfFLxU0v8fUws/6r2WOpHCo1m3KtTaXmgsrNplxrs9m5NJ0iIuJhKnEREQ+r9BJ/vNwBLqNSsynX2lRqLqjcbMq1Npuaq6LnxEVE5PIqfU1cREQuoyJL3MzCZva0mb1iZt+0cpzg7tJM3zCzA2b2lJndYWbnzOy54n/XlynTyhy3VMpyM7P3L8v1tpn9p3IvMzMLmtn3ipcvGWPlGnfLcxV/Xj7WAuUabyuW1yUZyvk6XZFt5Vj7TDmW2YrnLbZV46siS5wKO2a5md0DBJxzdwJxoAv4mnPunuJ/b5YpWtPyHMAdVMhyc879fFmuV4EJyrjMzCwCHOSdZbLaGNvycbcy1ypj7WFWPM9bsexWWV6rZSjL63RltlXG2qF3ybuZmVY+b59li8ZXpZZ4pR2zfAT4SvGyj8Un4BNm9oKZ/V0Z13gvygE8SGUtN8ysHtjN4jIs2zJzzqWcczcD54pXrTbGtnzcrZJr5ViDMoy3VXKtlqEsr9NVsgHvjDXn3KvvknczrXze/owtGl+VWuIVdcxy59xx59wLZvYxoAC8AfwH59x7WVwrv69M0d5akePjVNByK3qIxYOkrcxarmV2wWpjrOzjbpWx9mMqY9mtlqHsy2uFC2MNtniZrfK8HWKLxtfaz5S6Na54zPKtZmYfAb4AfBgIAS8XbzoNtJcp1mng8LLLt1Fhy43F5fUEl2Yt1zK7YLUxFlvlui23fKw553JmdpryL7vVMlTa6/TCWIMyjLcVHfE/2aLxValr4lc8ZvlWMrNO4EvAo865GeCLwCfNzAfcyDuDZautzPHvqKzlZsD7WfzoWCnL7ILVxljZx90qYw0qY9mtlqHsy+uCFWMNtniZrfK8bdn4qtQSr7Rjln+GxY9kPzKz54B54PeBfwSedM4dKVOury7PAfwVlbXc7gCOOOfSrMhaxmV2wWpjrBLG3UVjzcw+S2Usu9UyVMLyumD5WIOtX2YrOyLIFo0v7ewjIuJhlbomLiIiV0ElLiLiYSpxEREPU4mLiHiYSlxExMNU4iIiHqYSFxHxsP8PdLsJari0X9wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.model(x, w, b)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46087031, -0.35847379]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01337461131363381"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Loss持续下降，意味着什么呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(y, yhat):\n",
    "    return -np.sum(y*np.log(yhat) + (1 - y)*np.log(1 - yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming: [L1, L2, L3, .. LN]\n",
    "\n",
    "if there is a model, randomly give label:1 randomly give label 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = np.array([1, 0, 1, 0, 1]) # 二分类\n",
    "predicate_1 = np.array([0.5, 0.5, 0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4657359027997265"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss(true_label, predicate_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 瞎猜的时候的准确度，loss值，我们称为这个模型的Baseline\n",
    "## 你的值，最起码要比这个好~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Loss一直在下降，但是我们现在想知道的是，有多少个label预测对了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicated_labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM: 5.85, LSTAT: 8.77, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.345, LSTAT: 4.97, EXPENSIVE: 0, Predicated: 1\n",
      "RM: 6.389, LSTAT: 9.62, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 8.398, LSTAT: 5.91, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.273, LSTAT: 6.78, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.926, LSTAT: 13.59, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 6.728, LSTAT: 4.5, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.731, LSTAT: 13.61, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.404, LSTAT: 13.28, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.03, LSTAT: 7.88, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.52, LSTAT: 7.26, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 7.489, LSTAT: 1.73, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.031, LSTAT: 7.83, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.121, LSTAT: 8.44, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.82, LSTAT: 3.57, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.458, LSTAT: 12.6, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.426, LSTAT: 7.2, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.487, LSTAT: 5.9, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.317, LSTAT: 13.99, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.212, LSTAT: 17.6, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.759, LSTAT: 14.13, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.794, LSTAT: 21.24, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.273, LSTAT: 6.78, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.896, LSTAT: 24.39, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.203, LSTAT: 9.59, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 5.983, LSTAT: 18.07, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.545, LSTAT: 21.08, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.279, LSTAT: 11.97, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.765, LSTAT: 7.56, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.229, LSTAT: 12.87, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.287, LSTAT: 4.08, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.431, LSTAT: 5.08, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.52, LSTAT: 24.56, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.923, LSTAT: 3.16, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.229, LSTAT: 15.55, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.642, LSTAT: 9.69, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 6.382, LSTAT: 10.36, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.208, LSTAT: 15.17, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.693, LSTAT: 17.19, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.327, LSTAT: 11.25, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 6.727, LSTAT: 5.29, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 7.412, LSTAT: 5.25, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.051, LSTAT: 18.76, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.016, LSTAT: 2.96, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.015, LSTAT: 12.86, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.115, LSTAT: 9.43, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.757, LSTAT: 17.31, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.209, LSTAT: 7.14, EXPENSIVE: 0, Predicated: 1\n",
      "RM: 6.358, LSTAT: 11.22, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.895, LSTAT: 10.56, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.86, LSTAT: 6.92, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.481, LSTAT: 6.36, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.952, LSTAT: 17.15, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.813, LSTAT: 14.81, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.49, LSTAT: 5.98, EXPENSIVE: 0, Predicated: 1\n",
      "RM: 6.897, LSTAT: 11.38, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.726, LSTAT: 8.05, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.572, LSTAT: 14.69, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.565, LSTAT: 9.51, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 6.341, LSTAT: 17.79, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.362, LSTAT: 10.19, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.762, LSTAT: 10.42, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.169, LSTAT: 5.81, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.545, LSTAT: 21.08, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 4.88, LSTAT: 30.62, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.951, LSTAT: 17.92, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.096, LSTAT: 20.34, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.031, LSTAT: 7.83, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.713, LSTAT: 22.6, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.593, LSTAT: 9.67, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.155, LSTAT: 4.82, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 7.802, LSTAT: 1.92, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.981, LSTAT: 11.65, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 7.82, LSTAT: 3.57, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.093, LSTAT: 29.68, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.274, LSTAT: 6.62, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.604, LSTAT: 4.38, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.31, LSTAT: 6.75, EXPENSIVE: 0, Predicated: 1\n",
      "RM: 6.398, LSTAT: 7.79, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.437, LSTAT: 14.36, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.834, LSTAT: 8.47, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 8.725, LSTAT: 4.63, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.936, LSTAT: 16.94, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 5.976, LSTAT: 19.01, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.606, LSTAT: 7.37, EXPENSIVE: 0, Predicated: 1\n",
      "RM: 5.362, LSTAT: 10.19, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.155, LSTAT: 4.82, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.153, LSTAT: 13.15, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 5.304, LSTAT: 24.91, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.976, LSTAT: 5.64, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.579, LSTAT: 5.49, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.004, LSTAT: 17.1, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 7.412, LSTAT: 5.25, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 7.104, LSTAT: 8.05, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.781, LSTAT: 7.67, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.98, LSTAT: 11.66, EXPENSIVE: 1, Predicated: 0\n",
      "RM: 6.812, LSTAT: 4.85, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 6.12, LSTAT: 9.08, EXPENSIVE: 0, Predicated: 0\n",
      "RM: 6.579, LSTAT: 5.49, EXPENSIVE: 1, Predicated: 1\n",
      "RM: 5.362, LSTAT: 10.19, EXPENSIVE: 0, Predicated: 0\n"
     ]
    }
   ],
   "source": [
    "random_test_indices = np.random.choice(range(len(rm)), size=100)\n",
    "\n",
    "decision_boundary = 0.1  # threshold 阈值\n",
    "\n",
    "# 警察A: threshold: 0.5, precision: 1.b\n",
    "# 警察B: threshold: 0.1, precision: 1.\n",
    "# 警察C: threshold: 0.9, precision: 1.\n",
    "\n",
    "for i in random_test_indices:\n",
    "    x1, x2, y = rm[i], lstat[i], target[i]\n",
    "    predicate = model(np.array([x1, x2]), w, b)  # predicate in (0, 1)\n",
    "    predicate_label = int(predicate > decision_boundary)\n",
    "\n",
    "    print('RM: {}, LSTAT: {}, EXPENSIVE: {}, Predicated: {}'.format(x1, x2, y, predicate_label))\n",
    "    \n",
    "    true_labels.append(y)\n",
    "    predicated_labels.append(predicate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ytrues, ylabels):\n",
    "    return sum(1 for yt, y in zip(ytrues, ylabels) if yt == y) / len(ytrues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(true_labels, predicated_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 很容易出错~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assuming 这个场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个警察局，100个人里边，判断谁是犯罪分子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实情况：100个人里边，只有3个是犯罪分子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如警察A，他就说，这100个人，全部都是犯罪分子，accuracy有多少？\n",
    "\n",
    "假如警察B，他就说，这100个人，全部都不是犯罪分子，accuracy有多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(ytrues, yhats):\n",
    "    # 预测标签是 1的里边，正确的比例是多少\n",
    "    # 阳性，positive 检测的对象\n",
    "    \n",
    "    positives_pred = [y for y in yhats if y == 1]\n",
    "    \n",
    "    return sum(1 for yt, y in zip(ytrues, yhats) if yt == y and y == 1) / len(positives_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(true_labels, predicated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(ytrues, yhats):\n",
    "    # 实际是 positive 的里边，有多少比例的被找到了\n",
    "    \n",
    "    true_positive = [y for y in ytrues if y == 1] \n",
    "    \n",
    "    return sum(1 for yt, y in zip(ytrues, yhats) if yt == y and yt == 1) / len(true_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7954545454545454"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(true_labels, predicated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [ 0 ] * 90 + [1] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "警察1的结果 = [0] * 100\n",
    "警察2的结果 = [1] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(people, 警察1的结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-aad31acf209b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m警察1的结果\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-4a252ba3bcb2>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(ytrues, yhats)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpositives_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myhats\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositives_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "precision(people, 警察1的结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(people, 警察1的结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(people, 警察2的结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(people, 警察2的结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(people, 警察2的结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51,  5],\n",
       "       [ 9, 35]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(true_labels, predicated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "??confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归问题怎么评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ acc(y, \\hat{y}) = \\sum_{i \\in N} |y_i - \\hat{y_i}| $$\n",
    "$$ acc(y, \\hat{y}) = \\sum_{i \\in N} |y_i - \\hat{y_i}|^2 $$\n",
    "$$ acc(y, \\hat{y}) = \\sum_{i \\in N} \\frac{|y_i - \\hat{y_i}|}{|y_i|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2:  第一种情况：如何所有的yi和yhat-i的值都相等\n",
    "#### R2：第二种情况：如果所有的yhat-i是yi的平均值呢？\n",
    "#### R2：第三种情况：如果R2的值 比0还小，那么意味着什么呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: 在训练数据的时候，效果很好：loss很小，precision 很高，accuracy（当且仅当数据label比较均衡的时候，才有必要使用acc）也比较好，但是在实际情况下，用到没有见过的数据的时候，效果就很差了~  ==》 过拟合！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlinear 异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见的一种方法是用百分位来解决数值型问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = [0.9, 0.1, .... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(lstat) < np.percentile(np.array(lstat), 0.25) / 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  17,  18,\n",
       "         19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,\n",
       "         32,  33,  34,  35,  36,  37,  38,  43,  44,  45,  46,  47,  48,\n",
       "         49,  50,  51,  53,  54,  59,  60,  61,  63,  64,  66,  67,  68,\n",
       "         69,  71,  73,  75,  76,  77,  78,  79,  81,  83,  84,  86,  87,\n",
       "         90,  91,  92,  94,  96, 100, 101, 102, 103, 104, 105, 106, 107,\n",
       "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
       "        121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
       "        134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "        147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 159, 164, 165,\n",
       "        167, 168, 169, 170, 171, 172, 173, 174, 176, 180, 181, 184, 185,\n",
       "        197, 201, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "        216, 217, 218, 219, 220, 221, 222, 223, 230, 234, 235, 236, 239,\n",
       "        240, 241, 242, 244, 245, 246, 247, 248, 255, 258, 260, 261, 263,\n",
       "        264, 265, 266, 267, 269, 270, 272, 278, 284, 285, 286, 287, 288,\n",
       "        289, 293, 294, 296, 297, 301, 302, 305, 307, 309, 310, 312, 313,\n",
       "        314, 315, 316, 317, 318, 319, 320, 322, 323, 327, 328, 329, 330,\n",
       "        331, 332, 335, 336, 337, 338, 339, 340, 342, 343, 345, 346, 352,\n",
       "        354, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 367, 371,\n",
       "        372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384,\n",
       "        385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
       "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410,\n",
       "        411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "        424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
       "        437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449,\n",
       "        450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488,\n",
       "        489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501,\n",
       "        502, 505]),)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(lstat) > np.percentile(np.array(lstat), 0.75) * 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
