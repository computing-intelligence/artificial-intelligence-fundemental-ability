{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-bba56ab16aca>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-bba56ab16aca>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    w_dim = layers_dims[l] * layers_dims[l - 1]\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def dictionary_to_vector(params_dict):\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    k = 0\n",
    "    for l in range(1, L):\n",
    "    # Create temp variable to store dimension used on each layer\n",
    "    w_dim = layers_dims[l] * layers_dims[l - 1]\n",
    "    b_dim = layers_dims[l]\n",
    "    # Create temp var to be used in slicing parameters vector\n",
    "    temp_dim = k + w_dim\n",
    "    # add parameters to the dictionary\n",
    "    parameters[\"W\" + str(l)] = vector[ k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1]) \n",
    "    parameters[\"b\" + str(l)] = vector[ temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n",
    "    k += w_dim + b_dim\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "# Get the number of indices for the gradients to iterate over valid_grads = [key for key in gradients.keys() if not key.startswith(\"dA\")]\n",
    "    L = len(valid_grads)// 2\n",
    "    count = 0\n",
    "    # Iterate over all gradients and append them to new_grads list\n",
    "    for l in range(1, L + 1):\n",
    "    if count == 0:\n",
    "    new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n",
    "    new_grads = np.concatenate((new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
    "    else:\n",
    "    new_grads = np.concatenate((new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n",
    "    new_grads = np.concatenate( (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
    "    count += 1\n",
    "    return new_grads\n",
    "\n",
    "def forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn=\"tanh\"):\n",
    "# Compute forward prop\n",
    "    AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn) # Compute cost\n",
    "    cost = compute_cost(AL, Y)\n",
    "    return cost \n",
    "\n",
    "\n",
    "def gradient_check( parameters, gradients, X, Y, layers_dims, epsilon=1e-7, hidden_layers_activation_fn=\"tanh\"):\n",
    "    # Roll out parameters and gradients dictionaries\n",
    "    parameters_vector = dictionary_to_vector(parameters) gradients_vector = gradients_to_vector(gradients)\n",
    "    # Create vector of zeros to be used with epsilon\n",
    "    grads_approx = np.zeros_like(parameters_vector)\n",
    "    for i in range(len(parameters_vector)):\n",
    "    # Compute cost of theta + epsilon\n",
    "    theta_plus = np.copy(parameters_vector)\n",
    "    theta_plus[i] = theta_plus[i] + epsilon\n",
    "    j_plus = forward_prop_cost( X, vector_to_dictionary(theta_plus, layers_dims), Y, hidden_layers_activation_fn)\n",
    "    # Compute cost of theta - epsilon\n",
    "    theta_minus = np.copy(parameters_vector)\n",
    "    theta_minus[i] = theta_minus[i] - epsilon\n",
    "    j_minus = forward_prop_cost( X, vector_to_dictionary(theta_minus, layers_dims), Y, hidden_layers_activation_fn)\n",
    "    # Compute numerical gradients\n",
    "    grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n",
    "    # Compute the difference of numerical and analytical gradients numerator = norm(gradients_vector - grads_approx)\n",
    "    denominator = norm(grads_approx) + norm(gradients_vector)\n",
    "    difference = numerator / denominator\n",
    "    if difference > 10e-7:\n",
    "    print (\"\\033[31mThere is a mistake in back-propagation \" +\\ \"implementation. The difference is: {}\".format(difference))\n",
    "    else:\n",
    "    print (\"\\033[32mThere implementation of back-propagation is fine! \"+\\ \"The difference is: {}\".format(difference))\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S_{\\frac{\\partial{loss}}{{\\partial{w}}}} = \\beta S_{\\frac{\\partial{loss}}{{\\partial{w}}}} + (1 - \\beta) ||{\\frac{\\partial{loss}}{{\\partial{w}}}}||^2 $$\n",
    "$$ S_{\\frac{\\partial{loss}}{{\\partial{b}}}} = \\beta S_{\\frac{\\partial{loss}}{{\\partial{b}}}} + (1 - \\beta) ||{\\frac{\\partial{loss}}{{\\partial{b}}}}||^2 $$\n",
    "$$ w = w - \\alpha \\frac {\\frac{\\partial{loss}}{\\partial{w}}} {\\sqrt{S_{\\frac{\\partial{loss}}{{\\partial{w}}}}}} $$\n",
    "$$ b = b - \\alpha \\frac {\\frac{\\partial{loss}}{\\partial{b}}} {\\sqrt{S_{\\frac{\\partial{loss}}{{\\partial{b}}}}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V_{dw} = \\beta_1 V_{dw} + (1 - \\beta _ 1) dW $$\n",
    "$$ V_{db} = \\beta_1 V_{db} + (1 - \\beta _ 1) db $$\n",
    "$$ S_{dw} = \\beta_2 S_{dw} + (1 - \\beta _ 2) ||dw||^2 $$\n",
    "$$ S_{db} = \\beta_2 S_{db} + (1 - \\beta _ 2) ||db||^2 $$\n",
    "\n",
    "$$ V_{dw}^{corrected} = \\frac{V_{dw}} {1 - \\beta_1^{t}} $$ \n",
    "$$ V_{db}^{corrected} = \\frac{V_{db}} {1 - \\beta_1^{t}} $$ \n",
    "\n",
    "$$ S_{dw}^{corrected} = \\frac{S_{dw}} {1 - \\beta_2^{t}} $$ \n",
    "$$ S_{db}^{corrected} = \\frac{S_{db}} {1 - \\beta_2^{t}} $$ \n",
    "\n",
    "$$ W = W - \\alpha \\frac{V_{dw}^{corrected} } {\\sqrt{S_{dw}^{corrected}} + \\epsilon} $$\n",
    "$$ b = b - \\alpha \\frac{V_{db}^{corrected} } {\\sqrt{S_{db}^{corrected}} + \\epsilon} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
